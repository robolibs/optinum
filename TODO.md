# OPTINUM - Optimization & Numerics Library

> **HEADER-ONLY LIBRARY** - No compilation required, just `#include <optinum/optinum.hpp>`

---

## Native SIMD Math Implementation (Complete)

### Overview
High-performance vectorized math functions with no external dependencies.
Accuracy: ~3-5 ULP (good for ML/graphics, not scientific computing).

### Benchmark Results (Current)
```
| Function  | SLEEF Speedup | Our fast_* Speedup | Our Advantage |
|-----------|---------------|---------------------|---------------|
| exp       | 3.65x         | 6.33x               | 1.7x faster   |
| log       | 1.85x         | 4.32x               | 2.3x faster   |
| sin       | 8.89x         | 16.93x              | 1.9x faster   |
| cos       | 7.48x         | 15.81x              | 2.1x faster   |
| tanh      | 7.28x         | 27.16x              | 3.7x faster   |
| pow       | 1.07x         | 2.90x               | 2.7x faster   |
| sqrt      | N/A           | 3.48x               | Native SIMD   |
```

### Implementation Plan

#### Phase 0: Architecture Refactor to pack<T,W> [IN PROGRESS]

**Goal:** Port all fast_* functions from old SIMDVec API to new clean pack<T,W> API

| Old File | New File | Status | Notes |
|----------|----------|--------|-------|
| `fast_exp.hpp` (SIMDVec) | `exp.hpp` (pack) | âœ… DONE | Renamed fast_exp_new â†’ exp, 7.95x speedup |
| `fast_log.hpp` (SIMDVec) | `log.hpp` (pack) | ğŸš§ TODO | Port to pack<T,W> |
| `fast_trig.hpp` (SIMDVec sin/cos) | `sin.hpp`, `cos.hpp` (pack) | ğŸš§ TODO | Split into separate files |
| `fast_hyp.hpp` (SIMDVec tanh) | `tanh.hpp` (pack) | ğŸš§ TODO | Port to pack<T,W> |
| `fast_pow.hpp` (SIMDVec pow/sqrt) | `pow.hpp`, `sqrt.hpp` (pack) | ğŸš§ TODO | Split into separate files |

**Naming convention:**
- Old API: `fast_exp`, `fast_log`, etc. (will be deleted later)
- New API: `exp`, `log`, `sin`, `cos` (clean names in pack<T,W>)

**Steps:**
1. âœ… Rename `fast_exp_new.hpp` â†’ `exp.hpp` and update to use `simd::exp()`
2. â³ Port `fast_log` â†’ `log.hpp` using pack<float,4/8> and pack<double,2/4>
3. â³ Port `fast_sin/cos` â†’ `sin.hpp`/`cos.hpp`
4. â³ Port `fast_tanh` â†’ `tanh.hpp`
5. â³ Port `fast_pow` â†’ `pow.hpp`
6. â³ Port `fast_sqrt` â†’ `sqrt.hpp`
7. â³ Add all functions to `algo/transform.hpp`
8. â³ Comprehensive benchmarks vs old API
9. â³ Delete old fast_* files

#### Phase A: Core Functions (Priority Order)

| Function | Difficulty | Algorithm | Notes |
|----------|------------|-----------|-------|
| **exp** | Easy | Range reduction + polynomial | âœ… DONE: `exp.hpp` with pack<T,W> |
| **log** | Easy | Range reduction + polynomial | Use `log(x) = log(2^n * m) = n*ln2 + log(m)` |
| **sin/cos** | Medium | Payne-Hanek reduction + polynomial | Reduce to [-pi/4, pi/4], use Taylor/Chebyshev |
| **tan** | Medium | `sin/cos` ratio | After sin/cos work |
| **tanh** | Easy | `(exp(2x)-1)/(exp(2x)+1)` or polynomial | Can use fast_exp |
| **pow** | Medium | `exp(y * log(x))` | Compose exp + log |
| **sqrt** | Easy | Newton-Raphson + `_mm256_rsqrt_ps` | Hardware has rsqrt seed |
| **cbrt** | Medium | Newton-Raphson | Similar to sqrt |

#### Phase B: Inverse Trig (Lower Priority)
| Function | Difficulty | Notes |
|----------|------------|-------|
| asin | Medium | Polynomial on reduced range |
| acos | Easy | `pi/2 - asin(x)` |
| atan | Medium | Polynomial + range reduction |
| atan2 | Medium | Quadrant handling + atan |

#### Phase C: Hyperbolic (Use exp-based)
| Function | Algorithm |
|----------|-----------|
| sinh | `(exp(x) - exp(-x)) / 2` |
| cosh | `(exp(x) + exp(-x)) / 2` |
| asinh | `log(x + sqrt(x^2 + 1))` |
| acosh | `log(x + sqrt(x^2 - 1))` |
| atanh | `0.5 * log((1+x)/(1-x))` |

### File Structure
```
include/optinum/simd/math/
â”œâ”€â”€ simd_math.hpp          # Public API (includes all headers)
â”œâ”€â”€ detail/
â”‚   â”œâ”€â”€ map.hpp            # Scalar fallback (existing)
â”‚   â””â”€â”€ constants.hpp      # DONE: Math constants (LN2, PI, coefficients)
â”œâ”€â”€ fast_exp.hpp           # DONE: exp for float (AVX + SSE4.1)
â”œâ”€â”€ fast_log.hpp           # DONE: log for float (AVX + SSE4.1)
â”œâ”€â”€ fast_trig.hpp          # DONE: sin, cos for float (AVX + SSE4.1)
â”œâ”€â”€ fast_hyp.hpp           # DONE: tanh, sinh, cosh (AVX + SSE4.1)
â”œâ”€â”€ fast_pow.hpp           # DONE: pow, sqrt, rsqrt, cbrt, powi (AVX + SSE4.1)
â”œâ”€â”€ fast_inv_trig.hpp      # TODO: asin, acos, atan, atan2
â”œâ”€â”€ exponential.hpp        # Scalar fallback (template)
â”œâ”€â”€ trig.hpp               # Scalar fallback (template)
â”œâ”€â”€ hyperbolic.hpp         # Scalar fallback (template)
â””â”€â”€ pow.hpp                # Scalar fallback (template)
```

### Algorithm Details

#### exp(x) - DONE
```
1. Clamp x to [-88, 88] (avoid overflow/underflow)
2. n = round(x / ln2)           -- integer part
3. r = x - n * ln2              -- fractional part in [-ln2/2, ln2/2]  
4. exp(r) â‰ˆ 1 + r + rÂ²/2 + rÂ³/6 + râ´/24 + râµ/120  (polynomial)
5. result = exp(r) * 2^n        -- scale by power of 2 (bit manipulation)
```

#### log(x) - TODO
```
1. Extract exponent: x = 2^n * m, where m in [1, 2)
2. Normalize: m' = (m - 1) / (m + 1), maps to [-1/3, 1/3]
3. log(m) â‰ˆ 2 * (m' + m'^3/3 + m'^5/5 + ...)  (polynomial in m'^2)
4. result = n * ln2 + log(m)
```

#### sin(x) / cos(x) - TODO  
```
1. Range reduction: x' = x mod 2Ï€, then to [-Ï€, Ï€]
2. Further reduce to [-Ï€/4, Ï€/4] using symmetry:
   - sin(x) = cos(Ï€/2 - x)
   - sin(x + Ï€) = -sin(x)
   - cos(x + Ï€/2) = -sin(x)
3. Polynomial approximation (minimax coefficients):
   sin(x) â‰ˆ x - xÂ³/6 + xâµ/120 - xâ·/5040 + ...
   cos(x) â‰ˆ 1 - xÂ²/2 + xâ´/24 - xâ¶/720 + ...
```

#### tanh(x) - TODO (Easy using exp)
```
For |x| < 0.625:
  tanh(x) â‰ˆ x * (1 - xÂ²/3 + 2xâ´/15 - ...)  (polynomial)
For |x| >= 0.625:
  tanh(x) = (exp(2x) - 1) / (exp(2x) + 1)
  Or: tanh(x) = 1 - 2/(exp(2x) + 1)  (more stable for large x)
```

### Implementation Checklist

- [x] `fast_exp.hpp` - exp for float (AVX + SSE4.1)
- [x] `detail/constants.hpp` - Centralize all magic numbers
- [x] `fast_log.hpp` - log for float (AVX + SSE4.1)
- [x] `fast_trig.hpp` - sin, cos for float (AVX + SSE4.1)
- [x] `fast_hyp.hpp` - tanh, sinh, cosh (using fast_exp)
- [x] `fast_pow.hpp` - pow, sqrt, rsqrt, cbrt, powi
- [ ] Add double precision to fast_* functions
- [ ] `detail/poly.hpp` - Horner's method, Estrin's method helpers
- [ ] Add accuracy tests (compare against std:: with tolerance)
- [ ] Add AVX-512 variants
- [ ] Add NEON (ARM) variants
- [ ] Implement `fast_inv_trig.hpp` (asin, acos, atan, atan2)

### Testing Strategy
```cpp
// For each function, test:
// 1. Accuracy: |fast_f(x) - std::f(x)| < tolerance (e.g., 1e-5 for float)
// 2. Edge cases: 0, Â±inf, NaN, denormals
// 3. Range boundaries: overflow/underflow thresholds
// 4. Performance: benchmark vs scalar
```

### References
- Cephes library: https://www.netlib.org/cephes/
- "Elementary Functions" by Muller (textbook)
- Intel Intrinsics Guide: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/

---

## Overview

**optinum** is a high-performance C++ header-only library combining:
- **SIMD-accelerated tensor operations** (inspired by [Fastor](https://github.com/romeric/Fastor))
- **Numerical optimization algorithms** (inspired by [ensmallen](https://github.com/mlpack/ensmallen))
- **POD data storage** via [datapod](../datapod) (`dp::` namespace)

```cpp
namespace optinum { ... }
namespace on = optinum;  // alias
```

---

## Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Code                                   â”‚
â”‚                                                                     â”‚
â”‚   #include <datapod/matrix.hpp>                                     â”‚
â”‚   #include <optinum/optinum.hpp>                                    â”‚
â”‚                                                                     â”‚
â”‚   dp::mat::matrix<float, 4, 4> A, B, C;                             â”‚
â”‚   on::lina::matmul(A, B, C);              // C = A * B              â”‚
â”‚   // OR with explicit SIMD views:                                   â”‚
â”‚   auto vA = on::simd::view<8>(A);                                   â”‚
â”‚   auto vB = on::simd::view<8>(B);                                   â”‚
â”‚   auto vC = on::simd::view<8>(C);                                   â”‚
â”‚   on::lina::matmul(vA, vB, vC);                                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          optinum                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   on::opti   â”‚â”€â–¶â”‚   on::lina   â”‚â”€â–¶â”‚        on::simd          â”‚   â”‚
â”‚  â”‚ (optimizers) â”‚  â”‚ (linear alg) â”‚  â”‚  (views + algorithms)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                   â”‚                 â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                                          â”‚  simd::view<W>  â”‚        â”‚
â”‚                                          â”‚  (non-owning)   â”‚        â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚ points to
                                                    â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚            datapod                  â”‚
                              â”‚  dp::mat::vector, matrix, tensor    â”‚
                              â”‚  (owns memory, POD, serializable)   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Namespace Structure

```
datapod (dp)                         # DATA OWNERSHIP (external library)
â”œâ”€â”€ mat::scalar<T>                   # rank-0 (single value)
â”œâ”€â”€ mat::vector<T, N>                # rank-1 (1D array, aligned)
â”œâ”€â”€ mat::matrix<T, R, C>             # rank-2 (2D array, column-major, aligned)
â””â”€â”€ mat::tensor<T, Dims...>          # rank-N (N-D array)

optinum (on)                         # SIMD OPERATIONS (this library)
â”œâ”€â”€ simd        # Non-owning SIMD views + algorithms
â”‚   â”œâ”€â”€ pack<T, W>                   # SIMD register abstraction (W lanes)
â”‚   â”œâ”€â”€ mask<T, W>                   # comparison results, blend/select
â”‚   â”œâ”€â”€ Kernel<T, W, Rank>           # ptr + extents + strides + load/store
â”‚   â”œâ”€â”€ scalar_view<T, W>            # view over dp::mat::scalar
â”‚   â”œâ”€â”€ vector_view<T, W>            # view over dp::mat::vector
â”‚   â”œâ”€â”€ matrix_view<T, W>            # view over dp::mat::matrix
â”‚   â”œâ”€â”€ tensor_view<T, W, Rank>      # view over dp::mat::tensor
â”‚   â”œâ”€â”€ view<W>(dp_obj)              # factory: dp type -> simd view
â”‚   â”œâ”€â”€ algo::axpy, dot, norm, ...   # algorithms on views
â”‚   â”œâ”€â”€ math::exp, sin, cos, ...     # vectorized math
â”‚   â””â”€â”€ arch/                        # platform detection
â”‚
â”œâ”€â”€ lina        # Linear algebra operations (operate on dp types via views)
â”‚   â”œâ”€â”€ matmul, transpose, inverse   # matrix operations
â”‚   â”œâ”€â”€ lu, qr, svd, cholesky        # decompositions
â”‚   â”œâ”€â”€ solve, lstsq                 # linear solvers
â”‚   â”œâ”€â”€ einsum, contraction          # tensor algebra
â”‚   â””â”€â”€ norm, dot, cross             # vector operations
â”‚
â””â”€â”€ opti        # Numerical optimization
    â”œâ”€â”€ GradientDescent, SGD, Adam...
    â”œâ”€â”€ LBFGS, CMA-ES, PSO...
    â””â”€â”€ callbacks, schedulers...
```

---

## Architecture Graph

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              on::opti                                        â”‚
â”‚                         (Optimization Layer)                                 â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  gradient/  â”‚ â”‚  adaptive/  â”‚ â”‚quasi_newton/â”‚ â”‚     evolutionary/      â”‚  â”‚
â”‚  â”‚  gd, sgd    â”‚ â”‚ adam, rmsp  â”‚ â”‚   lbfgs     â”‚ â”‚  cmaes, de, pso, sa    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚               â”‚               â”‚                    â”‚               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                            core/                                    â”‚     â”‚
â”‚  â”‚            function, traits, callbacks, schedule, search            â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â”‚ uses
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              on::lina                                        â”‚
â”‚                       (Linear Algebra Layer)                                 â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   basic/    â”‚  â”‚  decompose/ â”‚  â”‚   solve/    â”‚  â”‚      algebra/       â”‚  â”‚
â”‚  â”‚matmul, transâ”‚  â”‚ lu, qr, svd â”‚  â”‚ solve, lstsqâ”‚  â”‚ einsum, contraction â”‚  â”‚
â”‚  â”‚ inv, det    â”‚  â”‚cholesky, eigâ”‚  â”‚             â”‚  â”‚ inner, outer, perm  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚               â”‚                     â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                            expr/                                        â”‚ â”‚
â”‚  â”‚              Expression templates, lazy evaluation, views               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â”‚ operates on
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              on::simd                                        â”‚
â”‚                      (SIMD Types + Primitives)                               â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         User-facing Types                            â”‚    â”‚
â”‚  â”‚    Scalar<T>           Vector<T, N>           Matrix<T, R, C>        â”‚    â”‚
â”‚  â”‚      (rank-0)            (rank-1)                (rank-2)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                          backend/                                    â”‚    â”‚
â”‚  â”‚     elementwise (add, sub, mul, div), reduce (sum, min, max)         â”‚    â”‚
â”‚  â”‚     dot, norm, matmul, transpose                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         intrinsic/                                   â”‚    â”‚
â”‚  â”‚            SIMDVec<T, Width> - CPU register abstraction              â”‚    â”‚
â”‚  â”‚                  SSE / AVX / AVX-512 / NEON                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                     â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     math/       â”‚  â”‚          arch/            â”‚  â”‚      meta/        â”‚   â”‚
â”‚  â”‚ sin,cos,exp,log â”‚  â”‚  platform, cpuid, macros  â”‚  â”‚ metaprogramming   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â”‚ wraps (composition)
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              datapod (dp::)                                  â”‚
â”‚                            (POD Data Storage)                                â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                         dp::mat:: types                              â”‚    â”‚
â”‚  â”‚  dp::mat::scalar<T>    dp::mat::vector<T,N>    dp::mat::matrix<T,R,C>â”‚    â”‚
â”‚  â”‚       (rank-0)               (rank-1)                 (rank-2)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  sequential.hpp â”‚  â”‚  adapters.hpp   â”‚  â”‚       spatial.hpp           â”‚   â”‚
â”‚  â”‚ Vector, String, â”‚  â”‚ Optional,Result â”‚  â”‚  Point, Pose, Quaternion    â”‚   â”‚
â”‚  â”‚ Queue, Stack    â”‚  â”‚ Variant, Pair   â”‚  â”‚  Velocity, State, Geo       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ associative.hpp â”‚  â”‚  temporal.hpp   â”‚                                    â”‚
â”‚  â”‚   Map, Set      â”‚  â”‚ Stamp,TimeSeriesâ”‚                                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SIMD Architecture (Bottom-Up)

This is how SIMD operations flow through the library layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           User API Layer                                    â”‚
â”‚                                                                             â”‚
â”‚   simd::Tensor<float, 8> a, b;                                              â”‚
â”‚   auto c = a + b;                    // Element-wise add                    â”‚
â”‚   auto d = lina::dot(a, b);          // Dot product                         â”‚
â”‚   simd::Matrix<float,4,4> M1, M2;                                           â”‚
â”‚   auto M3 = lina::matmul(M1, M2);    // Matrix multiply                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ calls
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Backend Layer                                     â”‚
â”‚                       (simd/backend/*.hpp)                                  â”‚
â”‚                                                                             â”‚
â”‚   backend::add<float, 8>(dst, src1, src2);    // Element-wise ops           â”‚
â”‚   backend::reduce_sum<float, 8>(src);          // Reductions                â”‚
â”‚   backend::dot<float, 8>(src1, src2);          // Dot product               â”‚
â”‚   backend::matmul<float,4,4,4>(dst, A, B);     // Matrix multiply           â”‚
â”‚                                                                             â”‚
â”‚   - Chooses best implementation based on size                               â”‚
â”‚   - Handles alignment, loop tiling, remainder                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ uses
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Intrinsic Layer                                     â”‚
â”‚                     (simd/intrinsic/*.hpp)                                  â”‚
â”‚                                                                             â”‚
â”‚   SIMDVec<float, 4> v1, v2;     // Wraps __m128 (SSE)                       â”‚
â”‚   SIMDVec<float, 8> v3, v4;     // Wraps __m256 (AVX)                       â”‚
â”‚   SIMDVec<float, 16> v5, v6;    // Wraps __m512 (AVX-512)                   â”‚
â”‚                                                                             â”‚
â”‚   auto v = SIMDVec<float,4>::load(ptr);                                     â”‚
â”‚   auto w = v1 + v2;                                                         â”‚
â”‚   auto s = v.hsum();            // Horizontal sum                           â”‚
â”‚   w.store(ptr);                                                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚ wraps
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Arch Layer                                        â”‚
â”‚                       (simd/arch/*.hpp)                                     â”‚
â”‚                                                                             â”‚
â”‚   OPTINUM_HAS_SSE, OPTINUM_HAS_AVX, OPTINUM_HAS_AVX512, OPTINUM_HAS_NEON    â”‚
â”‚   OPTINUM_SIMD_LEVEL = 128 / 256 / 512                                      â”‚
â”‚   OPTINUM_INLINE, OPTINUM_SIMD_ALIGN                                        â”‚
â”‚                                                                             â”‚
â”‚   #include <immintrin.h>  // x86                                            â”‚
â”‚   #include <arm_neon.h>   // ARM                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Current vs Target Implementation

### Current (Scalar Loops)
```cpp
// simd/tensor.hpp - Current implementation
constexpr Tensor& operator+=(const Tensor& rhs) noexcept {
    for (size_type i = 0; i < N; ++i)
        pod_[i] += rhs.pod_[i];  // One element at a time
    return *this;
}
```

### Target (SIMD Backend)
```cpp
// simd/tensor.hpp - Target implementation
Tensor& operator+=(const Tensor& rhs) noexcept {
    if constexpr (std::is_constant_evaluated()) {
        // Compile-time: scalar fallback (constexpr)
        for (size_type i = 0; i < N; ++i)
            pod_[i] += rhs.pod_[i];
    } else {
        // Runtime: SIMD backend
        backend::add<T, N>(data(), data(), rhs.data());
    }
    return *this;
}
```

---

## SIMDVec Type Mapping

| Type   | SSE (128-bit) | AVX (256-bit) | AVX-512 (512-bit) | NEON (128-bit) |
|--------|---------------|---------------|-------------------|----------------|
| float  | `__m128` (4)  | `__m256` (8)  | `__m512` (16)     | `float32x4_t` (4) |
| double | `__m128d` (2) | `__m256d` (4) | `__m512d` (8)     | `float64x2_t` (2) |
| int32  | `__m128i` (4) | `__m256i` (8) | `__m512i` (16)    | `int32x4_t` (4) |

---

## Folder Structure

```
include/optinum/
â”œâ”€â”€ optinum.hpp                      # Master header + namespace alias
â”‚
â”œâ”€â”€ simd/                            # on::simd namespace (Types + SIMD primitives)
â”‚   â”œâ”€â”€ simd.hpp                     # simd module header
â”‚   â”‚
â”‚   â”œâ”€â”€ arch/                        # Architecture detection [DONE]
â”‚   â”‚   â”œâ”€â”€ arch.hpp                 #   âœ“ SSE/AVX/AVX512/NEON detection
â”‚   â”‚   â””â”€â”€ macros.hpp               #   âœ“ OPTINUM_INLINE, alignment, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ meta/                        # Template metaprogramming
â”‚   â”‚   â””â”€â”€ meta.hpp                 #   pack_prod, type traits
â”‚   â”‚
â”‚   â”œâ”€â”€ intrinsic/                   # SIMD register abstraction
â”‚   â”‚   â”œâ”€â”€ simd_vec.hpp             #   SIMDVec<T, Width> main template
â”‚   â”‚   â”œâ”€â”€ sse.hpp                  #   SSE: float(__m128), double(__m128d)
â”‚   â”‚   â”œâ”€â”€ avx.hpp                  #   AVX: float(__m256), double(__m256d)
â”‚   â”‚   â”œâ”€â”€ avx512.hpp               #   AVX-512: float(__m512), double(__m512d)
â”‚   â”‚   â””â”€â”€ neon.hpp                 #   ARM NEON: float32x4_t, float64x2_t
â”‚   â”‚
â”‚   â”œâ”€â”€ math/                        # Vectorized math functions
â”‚   â”‚   â””â”€â”€ math.hpp                 #   sin, cos, exp, log, pow, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ backend/                     # SIMD operation implementations
â”‚   â”‚   â”œâ”€â”€ backend.hpp              #   Dispatcher header
â”‚   â”‚   â”œâ”€â”€ elementwise.hpp          #   add, sub, mul, div (vector ops)
â”‚   â”‚   â”œâ”€â”€ reduce.hpp               #   sum, min, max (reductions)
â”‚   â”‚   â”œâ”€â”€ dot.hpp                  #   Dot product
â”‚   â”‚   â”œâ”€â”€ norm.hpp                 #   L2 norm, normalize
â”‚   â”‚   â”œâ”€â”€ matmul.hpp               #   Matrix multiplication
â”‚   â”‚   â””â”€â”€ transpose.hpp            #   Matrix transpose
â”‚   â”‚
â”‚   â”œâ”€â”€ scalar.hpp                   # âœ“ Scalar<T> wraps dp::mat::scalar<T>
â”‚   â”œâ”€â”€ vector.hpp                   # âœ“ Vector<T,N> wraps dp::mat::vector<T,N> (1D)
â”‚   â”œâ”€â”€ matrix.hpp                   # âœ“ Matrix<T,R,C> wraps dp::mat::matrix<T,R,C>
â”‚   â”œâ”€â”€ tensor.hpp                   # âœ“ Tensor<T,Dims...> wraps dp::mat::tensor (N-D, rank >= 3)
â”‚   â””â”€â”€ traits.hpp                   #   Type traits for vectors/matrices
â”‚
â”œâ”€â”€ lina/                            # on::lina namespace (Linear Algebra Operations)
â”‚   â”œâ”€â”€ lina.hpp                     # lina module header
â”‚   â”‚
â”‚   â”œâ”€â”€ basic/                       # Basic matrix operations
â”‚   â”‚   â”œâ”€â”€ matmul.hpp               #   Matrix multiplication
â”‚   â”‚   â”œâ”€â”€ transpose.hpp            #   Transpose
â”‚   â”‚   â”œâ”€â”€ inverse.hpp              #   Matrix inverse
â”‚   â”‚   â”œâ”€â”€ determinant.hpp          #   Determinant
â”‚   â”‚   â”œâ”€â”€ trace.hpp                #   Trace
â”‚   â”‚   â””â”€â”€ norm.hpp                 #   Frobenius, L2, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ decompose/                   # Matrix decompositions
â”‚   â”‚   â”œâ”€â”€ lu.hpp                   #   LU factorization
â”‚   â”‚   â”œâ”€â”€ qr.hpp                   #   QR factorization
â”‚   â”‚   â”œâ”€â”€ svd.hpp                  #   Singular value decomposition
â”‚   â”‚   â”œâ”€â”€ cholesky.hpp             #   Cholesky decomposition
â”‚   â”‚   â””â”€â”€ eig.hpp                  #   Eigendecomposition
â”‚   â”‚
â”‚   â”œâ”€â”€ solve/                       # Linear solvers
â”‚   â”‚   â”œâ”€â”€ solve.hpp                #   Solve Ax = b
â”‚   â”‚   â””â”€â”€ lstsq.hpp                #   Least squares
â”‚   â”‚
â”‚   â”œâ”€â”€ algebra/                     # Tensor algebra
â”‚   â”‚   â”œâ”€â”€ einsum.hpp               #   Einstein summation
â”‚   â”‚   â”œâ”€â”€ contraction.hpp          #   Tensor contraction
â”‚   â”‚   â”œâ”€â”€ permute.hpp              #   Tensor permutation
â”‚   â”‚   â”œâ”€â”€ inner.hpp                #   Inner product
â”‚   â”‚   â””â”€â”€ outer.hpp                #   Outer product
â”‚   â”‚
â”‚   â””â”€â”€ expr/                        # Expression templates
â”‚       â”œâ”€â”€ abstract.hpp             #   CRTP base class
â”‚       â”œâ”€â”€ binary/                  #   Binary operations
â”‚       â”‚   â”œâ”€â”€ arithmetic.hpp       #     +, -, *, /
â”‚       â”‚   â”œâ”€â”€ compare.hpp          #     <, >, ==, !=
â”‚       â”‚   â””â”€â”€ math.hpp             #     pow, atan2, hypot
â”‚       â”œâ”€â”€ unary/                   #   Unary operations
â”‚       â”‚   â”œâ”€â”€ math.hpp             #     sqrt, abs, sin, cos
â”‚       â”‚   â””â”€â”€ bool.hpp             #     !, all_of, any_of
â”‚       â””â”€â”€ views/                   #   Tensor views/slices
â”‚           â”œâ”€â”€ view.hpp             #     1D/2D/ND views
â”‚           â”œâ”€â”€ diag.hpp             #     Diagonal view
â”‚           â””â”€â”€ filter.hpp           #     Boolean mask filter
â”‚
â””â”€â”€ opti/                            # on::opti namespace (Optimization)
    â”œâ”€â”€ opti.hpp                     # opti module header
    â”‚
    â”œâ”€â”€ core/                        # Core infrastructure
    â”‚   â”œâ”€â”€ function.hpp             #   Function wrapper with mixins
    â”‚   â”œâ”€â”€ traits.hpp               #   Function type traits
    â”‚   â”œâ”€â”€ checks.hpp               #   Static interface checks
    â”‚   â””â”€â”€ log.hpp                  #   Logging utilities
    â”‚
    â”œâ”€â”€ callback/                    # Optimization callbacks
    â”‚   â”œâ”€â”€ callback.hpp             #   Base callback infrastructure
    â”‚   â”œâ”€â”€ early_stop.hpp           #   Stop when loss plateaus
    â”‚   â”œâ”€â”€ grad_clip.hpp            #   Gradient clipping
    â”‚   â”œâ”€â”€ print.hpp                #   Print loss each iteration
    â”‚   â”œâ”€â”€ progress.hpp             #   Progress bar
    â”‚   â””â”€â”€ timer.hpp                #   Time-based stopping
    â”‚
    â”œâ”€â”€ gradient/                    # First-order methods
    â”‚   â”œâ”€â”€ gradient.hpp             #   Module header
    â”‚   â”œâ”€â”€ gd.hpp                   #   Gradient Descent
    â”‚   â”œâ”€â”€ sgd/                     #   SGD family
    â”‚   â”‚   â”œâ”€â”€ sgd.hpp              #     Stochastic GD
    â”‚   â”‚   â”œâ”€â”€ momentum.hpp         #     Momentum SGD
    â”‚   â”‚   â””â”€â”€ nesterov.hpp         #     Nesterov Momentum
    â”‚   â”œâ”€â”€ coordinate/              #   Coordinate descent
    â”‚   â”‚   â”œâ”€â”€ random.hpp           #     Random coordinate
    â”‚   â”‚   â”œâ”€â”€ cyclic.hpp           #     Cyclic coordinate
    â”‚   â”‚   â””â”€â”€ greedy.hpp           #     Greedy coordinate
    â”‚   â””â”€â”€ parallel/                #   Parallel methods
    â”‚       â””â”€â”€ hogwild.hpp          #     Hogwild! (lock-free SGD)
    â”‚
    â”œâ”€â”€ adaptive/                    # Adaptive learning rate
    â”‚   â”œâ”€â”€ adaptive.hpp             #   Module header
    â”‚   â”œâ”€â”€ adam/                    #   Adam family
    â”‚   â”‚   â”œâ”€â”€ adam.hpp             #     Adam
    â”‚   â”‚   â”œâ”€â”€ adamax.hpp           #     AdaMax
    â”‚   â”‚   â”œâ”€â”€ amsgrad.hpp          #     AMSGrad
    â”‚   â”‚   â”œâ”€â”€ nadam.hpp            #     Nadam
    â”‚   â”‚   â””â”€â”€ padam.hpp            #     PAdam
    â”‚   â”œâ”€â”€ adagrad.hpp              #   AdaGrad
    â”‚   â”œâ”€â”€ adadelta.hpp             #   AdaDelta
    â”‚   â”œâ”€â”€ rmsprop.hpp              #   RMSProp
    â”‚   â”œâ”€â”€ adabelief.hpp            #   AdaBelief
    â”‚   â”œâ”€â”€ adabound.hpp             #   AdaBound
    â”‚   â”œâ”€â”€ yogi.hpp                 #   Yogi
    â”‚   â”œâ”€â”€ eve.hpp                  #   Eve
    â”‚   â”œâ”€â”€ swats.hpp                #   SWATS (Adam-to-SGD)
    â”‚   â””â”€â”€ lookahead.hpp            #   Lookahead wrapper
    â”‚
    â”œâ”€â”€ variance/                    # Variance reduction
    â”‚   â”œâ”€â”€ variance.hpp             #   Module header
    â”‚   â”œâ”€â”€ svrg.hpp                 #   SVRG
    â”‚   â”œâ”€â”€ sarah.hpp                #   SARAH / SARAH+
    â”‚   â””â”€â”€ katyusha.hpp             #   Katyusha
    â”‚
    â”œâ”€â”€ quasi_newton/                # Second-order methods
    â”‚   â”œâ”€â”€ quasi_newton.hpp         #   Module header
    â”‚   â”œâ”€â”€ lbfgs.hpp                #   L-BFGS
    â”‚   â””â”€â”€ iqn.hpp                  #   Incremental Quasi-Newton
    â”‚
    â”œâ”€â”€ proximal/                    # Proximal methods
    â”‚   â”œâ”€â”€ proximal.hpp             #   Module header
    â”‚   â”œâ”€â”€ fbs.hpp                  #   Forward-Backward Splitting
    â”‚   â”œâ”€â”€ fista.hpp                #   FISTA
    â”‚   â”œâ”€â”€ fasta.hpp                #   FASTA
    â”‚   â””â”€â”€ frankwolfe/              #   Frank-Wolfe / Conditional gradient
    â”‚       â”œâ”€â”€ frankwolfe.hpp       #     Frank-Wolfe optimizer
    â”‚       â”œâ”€â”€ atoms.hpp            #     Atom dictionary
    â”‚       â””â”€â”€ constraint.hpp       #     Constraint types
    â”‚
    â”œâ”€â”€ constrained/                 # Constrained optimization
    â”‚   â”œâ”€â”€ constrained.hpp          #   Module header
    â”‚   â”œâ”€â”€ augmented.hpp            #   Augmented Lagrangian
    â”‚   â””â”€â”€ sdp/                     #   Semidefinite programming
    â”‚       â”œâ”€â”€ primal_dual.hpp      #     Primal-dual solver
    â”‚       â””â”€â”€ lrsdp.hpp            #     Low-rank SDP
    â”‚
    â”œâ”€â”€ evolutionary/                # Derivative-free / evolutionary
    â”‚   â”œâ”€â”€ evolutionary.hpp         #   Module header
    â”‚   â”œâ”€â”€ cmaes/                   #   CMA-ES family
    â”‚   â”‚   â”œâ”€â”€ cmaes.hpp            #     CMA-ES
    â”‚   â”‚   â”œâ”€â”€ active.hpp           #     Active CMA-ES
    â”‚   â”‚   â”œâ”€â”€ bipop.hpp            #     BIPOP-CMA-ES
    â”‚   â”‚   â””â”€â”€ ipop.hpp             #     IPOP-CMA-ES
    â”‚   â”œâ”€â”€ de.hpp                   #   Differential Evolution
    â”‚   â”œâ”€â”€ pso.hpp                  #   Particle Swarm Optimization
    â”‚   â”œâ”€â”€ sa.hpp                   #   Simulated Annealing
    â”‚   â”œâ”€â”€ spsa.hpp                 #   SPSA
    â”‚   â””â”€â”€ cne.hpp                  #   Conventional Neural Evolution
    â”‚
    â”œâ”€â”€ multiobjective/              # Multi-objective optimization
    â”‚   â”œâ”€â”€ multiobjective.hpp       #   Module header
    â”‚   â”œâ”€â”€ nsga2.hpp                #   NSGA-II
    â”‚   â”œâ”€â”€ agemoea.hpp              #   AGE-MOEA
    â”‚   â”œâ”€â”€ moead.hpp                #   MOEA/D
    â”‚   â””â”€â”€ indicator/               #   Quality indicators
    â”‚       â”œâ”€â”€ epsilon.hpp          #     Epsilon indicator
    â”‚       â”œâ”€â”€ igd.hpp              #     Inverted Generational Distance
    â”‚       â””â”€â”€ hypervolume.hpp      #     Hypervolume indicator
    â”‚
    â”œâ”€â”€ schedule/                    # Learning rate scheduling
    â”‚   â”œâ”€â”€ schedule.hpp             #   Module header
    â”‚   â”œâ”€â”€ cyclical.hpp             #   Cyclical LR (SGDR)
    â”‚   â”œâ”€â”€ warmup.hpp               #   Warm restarts
    â”‚   â””â”€â”€ adaptive.hpp             #   SPALeRA, Big Batch
    â”‚
    â”œâ”€â”€ search/                      # Hyperparameter search
    â”‚   â”œâ”€â”€ search.hpp               #   Module header
    â”‚   â””â”€â”€ grid.hpp                 #   Grid search
    â”‚
    â””â”€â”€ problem/                     # Benchmark functions
        â”œâ”€â”€ problem.hpp              #   Module header
        â”œâ”€â”€ unconstrained/           #   Single-objective test functions
        â”‚   â”œâ”€â”€ rosenbrock.hpp       #     Rosenbrock function
        â”‚   â”œâ”€â”€ sphere.hpp           #     âœ“ Sphere function
        â”‚   â”œâ”€â”€ rastrigin.hpp        #     Rastrigin function
        â”‚   â””â”€â”€ ackley.hpp           #     Ackley function
        â””â”€â”€ multiobjective/          #   Multi-objective test functions
            â”œâ”€â”€ dtlz/                #     DTLZ test suite
            â”œâ”€â”€ zdt/                 #     ZDT test suite
            â””â”€â”€ schaffer.hpp         #     Schaffer functions
```

---

## Implementation Roadmap

### Phase 0: SIMD Architecture Refactor - Non-Owning Views [TODO]

**Goal:** Refactor SIMD layer from owning wrappers to non-owning views over `dp::mat::*` types.

#### 0.0 Design Philosophy

**Current (problematic):**
```cpp
dp::mat::vector<float, 3> point;           // dp owns data
simd::Vector<float, 3> simd_point(point);  // COPIES data into simd wrapper
// ... do SIMD operations ...
point = simd_point.pod();                  // COPY back to dp
```

**New (zero-copy views):**
```cpp
dp::mat::vector<float, 1024> a, b, c;      // dp owns ALL data
simd::vector<float, 8> va = simd::view<8>(a);  // non-owning view (just ptr + metadata)
simd::vector<float, 8> vb = simd::view<8>(b);
simd::axpy(2.0f, va, vb, simd::view<8>(c)); // operates directly on dp memory
// No copy ever happens - c is already updated
```

**Type symmetry:**
```
dp::mat::scalar<T>       â†’  simd::scalar<T, W>       (non-owning view)
dp::mat::vector<T, N>    â†’  simd::vector<T, W>       (non-owning view)
dp::mat::matrix<T, R, C> â†’  simd::matrix<T, W>       (non-owning view)
dp::mat::tensor<T, ...>  â†’  simd::tensor<T, W, Rank> (non-owning view)
```

**Key principles:**
- `dp::mat::*` types own memory (POD, serializable, cache-aligned)
- `simd::*` types (scalar/vector/matrix/tensor) are non-owning views with SIMD operations
- Algorithms accept views, operate directly on dp memory
- Views are cheap, trivially copyable, stack-allocated

#### 0.1 New File Structure

```
simd/
â”œâ”€â”€ arch/                    # [KEEP] Platform detection (unchanged)
â”‚   â”œâ”€â”€ arch.hpp
â”‚   â””â”€â”€ macros.hpp
â”‚
â”œâ”€â”€ pack/                    # [NEW] SIMD register abstraction
â”‚   â”œâ”€â”€ pack.hpp             #   pack<T,W> - primary template + scalar fallback
â”‚   â”œâ”€â”€ pack_sse.hpp         #   SSE specializations (float4, double2, int32x4, int64x2)
â”‚   â”œâ”€â”€ pack_avx.hpp         #   AVX/AVX2 specializations
â”‚   â”œâ”€â”€ pack_avx512.hpp      #   AVX-512 specializations
â”‚   â””â”€â”€ pack_neon.hpp        #   ARM NEON specializations
â”‚
â”œâ”€â”€ mask.hpp                 # [NEW] mask<T,W> - comparison results, any/all, blend/select
â”‚
â”œâ”€â”€ kernel.hpp               # [NEW] Kernel<T,W,Rank> - ptr + extents + strides + load/store
â”‚
â”œâ”€â”€ scalar.hpp               # [REPLACE] simd::scalar<T,W> - non-owning view (rank-0)
â”œâ”€â”€ vector.hpp               # [REPLACE] simd::vector<T,W> - non-owning view (rank-1)
â”œâ”€â”€ matrix.hpp               # [REPLACE] simd::matrix<T,W> - non-owning view (rank-2)
â”œâ”€â”€ tensor.hpp               # [REPLACE] simd::tensor<T,W,Rank> - non-owning view (rank-N)
â”‚
â”œâ”€â”€ bridge.hpp               # [NEW] simd::view<W>(dp_obj) factory functions
â”‚
â”œâ”€â”€ algo/                    # [NEW] Algorithms on views
â”‚   â”œâ”€â”€ algo.hpp             #   Module header
â”‚   â”œâ”€â”€ elementwise.hpp      #   add, sub, mul, div, axpy, scale
â”‚   â”œâ”€â”€ reduce.hpp           #   sum, min, max, prod, dot, norm
â”‚   â”œâ”€â”€ transform.hpp        #   apply(f, view), map
â”‚   â””â”€â”€ linalg.hpp           #   matmul, transpose, solve (delegating to lina/)
â”‚
â”œâ”€â”€ math/                    # [KEEP] SIMD math functions (updated to use pack<T,W>)
â”‚   â”œâ”€â”€ simd_math.hpp
â”‚   â”œâ”€â”€ fast_exp.hpp
â”‚   â”œâ”€â”€ fast_log.hpp
â”‚   â”œâ”€â”€ fast_trig.hpp
â”‚   â”œâ”€â”€ fast_hyp.hpp
â”‚   â”œâ”€â”€ fast_pow.hpp
â”‚   â””â”€â”€ detail/
â”‚       â”œâ”€â”€ constants.hpp
â”‚       â””â”€â”€ map.hpp
â”‚
â””â”€â”€ simd.hpp                 # [UPDATE] Module header - exports views + bridge + algo
```

**Files to DELETE:**
- `simd/intrinsic/simd_vec.hpp` (replaced by pack/)
- `simd/intrinsic/sse.hpp` (replaced by pack_sse.hpp)
- `simd/intrinsic/avx.hpp` (replaced by pack_avx.hpp)
- `simd/intrinsic/avx512.hpp` (replaced by pack_avx512.hpp)
- `simd/intrinsic/neon.hpp` (replaced by pack_neon.hpp)
- `simd/backend/` (merged into algo/)

**Files to REPLACE (same name, new content):**
- `simd/scalar.hpp` - was owning Scalar<T>, becomes non-owning simd::scalar<T,W>
- `simd/vector.hpp` - was owning Vector<T,N>, becomes non-owning simd::vector<T,W>
- `simd/matrix.hpp` - was owning Matrix<T,R,C>, becomes non-owning simd::matrix<T,W>
- `simd/tensor.hpp` - was owning Tensor<T,Dims...>, becomes non-owning simd::tensor<T,W,Rank>

#### 0.2 Core Types

##### pack<T, W> - SIMD Register Abstraction
```cpp
namespace simd {

// Primary template (scalar fallback)
template <typename T, std::size_t W>
struct pack {
    static_assert(W > 0 && std::is_arithmetic_v<T>);
    using value_type = T;
    static constexpr std::size_t width = W;
    
    alignas(W * sizeof(T)) T data[W];
    
    // Construction
    pack() = default;
    explicit pack(T val);                    // broadcast
    
    // Load/Store
    static pack load(const T* p);            // aligned
    static pack loadu(const T* p);           // unaligned
    void store(T* p) const;
    void storeu(T* p) const;
    
    // Arithmetic
    pack operator+(pack rhs) const;
    pack operator-(pack rhs) const;
    pack operator*(pack rhs) const;
    pack operator/(pack rhs) const;
    pack operator-() const;
    
    // FMA
    static pack fma(pack a, pack b, pack c);  // a*b + c
    static pack fms(pack a, pack b, pack c);  // a*b - c
    
    // Math
    pack sqrt() const;
    pack rsqrt() const;
    pack abs() const;
    static pack min(pack a, pack b);
    static pack max(pack a, pack b);
    
    // Reductions
    T hsum() const;
    T hmin() const;
    T hmax() const;
    T hprod() const;
    
    // Bitwise (integer only)
    pack operator&(pack rhs) const;
    pack operator|(pack rhs) const;
    pack operator^(pack rhs) const;
    pack operator~() const;
    pack operator<<(int n) const;
    pack operator>>(int n) const;
    
    // Element access
    T operator[](std::size_t i) const;
};

// Specializations in pack_sse.hpp, pack_avx.hpp, etc.
// pack<float, 4>   -> __m128
// pack<float, 8>   -> __m256
// pack<float, 16>  -> __m512
// pack<double, 2>  -> __m128d
// pack<double, 4>  -> __m256d
// pack<double, 8>  -> __m512d
// pack<int32_t, 4/8/16> -> __m128i/__m256i/__m512i
// pack<int64_t, 2/4/8>  -> __m128i/__m256i/__m512i

} // namespace simd
```

##### mask<T, W> - Comparison Results
```cpp
namespace simd {

template <typename T, std::size_t W>
struct mask {
    // Internal representation varies by ISA:
    // - SSE/AVX: __m128/__m256 (all bits set per lane)
    // - AVX-512: __mmask8/__mmask16
    // - Scalar: std::array<bool, W>
    
    // Factory
    static mask all_true();
    static mask all_false();
    static mask first_n(std::size_t n);      // first n lanes true
    
    // Combine
    mask operator&(mask rhs) const;
    mask operator|(mask rhs) const;
    mask operator^(mask rhs) const;
    mask operator!() const;
    
    // Query
    bool all() const;                         // all lanes true
    bool any() const;                         // any lane true
    bool none() const;                        // no lane true
    int popcount() const;                     // count true lanes
    
    // Element access
    bool operator[](std::size_t i) const;
};

// Comparisons return mask
template <typename T, std::size_t W>
mask<T,W> cmp_eq(pack<T,W> a, pack<T,W> b);
mask<T,W> cmp_ne(pack<T,W> a, pack<T,W> b);
mask<T,W> cmp_lt(pack<T,W> a, pack<T,W> b);
mask<T,W> cmp_le(pack<T,W> a, pack<T,W> b);
mask<T,W> cmp_gt(pack<T,W> a, pack<T,W> b);
mask<T,W> cmp_ge(pack<T,W> a, pack<T,W> b);

// Masked operations
template <typename T, std::size_t W>
pack<T,W> blend(pack<T,W> a, pack<T,W> b, mask<T,W> m);  // m ? b : a
pack<T,W> maskload(const T* p, mask<T,W> m);
void maskstore(T* p, pack<T,W> v, mask<T,W> m);

} // namespace simd
```

##### Kernel<T, W, Rank> - Memory Layout Descriptor
```cpp
namespace simd {

template <typename T, std::size_t W, std::size_t Rank>
struct Kernel {
    T* ptr = nullptr;
    std::array<std::size_t, Rank> extents{};
    std::array<std::size_t, Rank> strides{};
    
    // Metadata
    constexpr std::size_t extent(std::size_t d) const { return extents[d]; }
    constexpr std::size_t stride(std::size_t d) const { return strides[d]; }
    constexpr std::size_t linear_size() const;
    constexpr std::size_t num_packs() const { return (linear_size() + W - 1) / W; }
    constexpr std::size_t tail_size() const { return linear_size() % W; }
    
    // Linear (contiguous) access
    pack<T,W> load_pack(std::size_t pack_idx) const;
    void store_pack(std::size_t pack_idx, pack<T,W> v) const;
    
    // Tail handling (last partial pack)
    pack<T,W> load_pack_tail(std::size_t pack_idx, std::size_t valid) const;
    void store_pack_tail(std::size_t pack_idx, pack<T,W> v, std::size_t valid) const;
    
    // Scalar access
    T& at_linear(std::size_t i) const { return ptr[i]; }
};

} // namespace simd
```

##### Views - Typed Wrappers Around Kernel
```cpp
namespace simd {

template <typename T, std::size_t W>
struct scalar_view {
    Kernel<T, W, 0> k;  // Rank-0: single element
    T& get() const { return *k.ptr; }
    operator T&() const { return get(); }
};

template <typename T, std::size_t W>
struct vector_view {
    Kernel<T, W, 1> k;  // Rank-1: 1D array
    
    std::size_t size() const { return k.extent(0); }
    T& operator[](std::size_t i) const { return k.at_linear(i); }
    
    // Pack access
    pack<T,W> load_pack(std::size_t i) const { return k.load_pack(i); }
    void store_pack(std::size_t i, pack<T,W> v) const { k.store_pack(i, v); }
    std::size_t num_packs() const { return k.num_packs(); }
    std::size_t tail_size() const { return k.tail_size(); }
};

template <typename T, std::size_t W>
struct matrix_view {
    Kernel<T, W, 2> k;  // Rank-2: 2D array (column-major)
    
    std::size_t rows() const { return k.extent(0); }
    std::size_t cols() const { return k.extent(1); }
    T& operator()(std::size_t r, std::size_t c) const;
    
    // Row/column views
    vector_view<T, W> row(std::size_t r) const;
    vector_view<T, W> col(std::size_t c) const;
};

template <typename T, std::size_t W, std::size_t Rank>
struct tensor_view {
    Kernel<T, W, Rank> k;
    
    template <typename... Idx>
    T& operator()(Idx... idx) const;
    
    std::size_t extent(std::size_t d) const { return k.extent(d); }
};

} // namespace simd
```

##### Bridge - Factory Functions for dp Types
```cpp
// bridge.hpp
namespace simd {

// Detect SIMD width for type
template <typename T>
constexpr std::size_t default_width() {
    if constexpr (std::is_same_v<T, float>) {
#if defined(OPTINUM_HAS_AVX)
        return 8;
#elif defined(OPTINUM_HAS_SSE)
        return 4;
#else
        return 4;  // scalar fallback, 4-wide
#endif
    } else if constexpr (std::is_same_v<T, double>) {
#if defined(OPTINUM_HAS_AVX)
        return 4;
#elif defined(OPTINUM_HAS_SSE)
        return 2;
#else
        return 2;
#endif
    }
    // ... int32_t, int64_t
}

// View factory: explicit width
template <std::size_t W, typename T, std::size_t N>
vector_view<T, W> view(dp::mat::vector<T, N>& v) {
    return vector_view<T, W>{
        Kernel<T, W, 1>{v.data(), {N}, {1}}
    };
}

template <std::size_t W, typename T, std::size_t N>
vector_view<const T, W> view(const dp::mat::vector<T, N>& v) {
    return vector_view<const T, W>{
        Kernel<const T, W, 1>{v.data(), {N}, {1}}
    };
}

template <std::size_t W, typename T, std::size_t R, std::size_t C>
matrix_view<T, W> view(dp::mat::matrix<T, R, C>& m) {
    // Column-major: stride(0)=1, stride(1)=R
    return matrix_view<T, W>{
        Kernel<T, W, 2>{m.data(), {R, C}, {1, R}}
    };
}

// View factory: auto width
template <typename T, std::size_t N>
auto view(dp::mat::vector<T, N>& v) {
    return view<default_width<T>()>(v);
}

} // namespace simd
```

#### 0.3 Algorithm Examples

```cpp
// algo/elementwise.hpp
namespace simd {

// axpy: y = a*x + y
template <typename T, std::size_t W>
void axpy(T alpha, vector_view<const T, W> x, vector_view<T, W> y) {
    const std::size_t n = x.size();
    const std::size_t npacks = n / W;
    const std::size_t tail = n % W;
    
    pack<T, W> a(alpha);
    
    for (std::size_t i = 0; i < npacks; ++i) {
        pack<T, W> xi = x.load_pack(i);
        pack<T, W> yi = y.load_pack(i);
        y.store_pack(i, pack<T,W>::fma(a, xi, yi));
    }
    
    if (tail > 0) {
        pack<T, W> xi = x.k.load_pack_tail(npacks, tail);
        pack<T, W> yi = y.k.load_pack_tail(npacks, tail);
        y.k.store_pack_tail(npacks, pack<T,W>::fma(a, xi, yi), tail);
    }
}

// dot: sum(x[i] * y[i])
template <typename T, std::size_t W>
T dot(vector_view<const T, W> x, vector_view<const T, W> y) {
    const std::size_t n = x.size();
    const std::size_t npacks = n / W;
    const std::size_t tail = n % W;
    
    pack<T, W> acc(T{0});
    
    for (std::size_t i = 0; i < npacks; ++i) {
        pack<T, W> xi = x.load_pack(i);
        pack<T, W> yi = y.load_pack(i);
        acc = pack<T,W>::fma(xi, yi, acc);
    }
    
    T result = acc.hsum();
    
    // Scalar tail
    for (std::size_t i = npacks * W; i < n; ++i) {
        result += x[i] * y[i];
    }
    
    return result;
}

} // namespace simd
```

#### 0.4 Usage Example (End-to-End)

```cpp
#include <datapod/matrix.hpp>
#include <optinum/simd/simd.hpp>

namespace dp = datapod;
namespace on = optinum;

int main() {
    // dp owns all data
    dp::mat::vector<float, 1024> x, y, z;
    dp::mat::matrix<float, 64, 64> A;
    
    // Initialize...
    for (int i = 0; i < 1024; ++i) {
        x[i] = static_cast<float>(i);
        y[i] = 1.0f;
    }
    
    // Get SIMD views (auto-detect width: 8 for float on AVX)
    auto vx = on::simd::view(x);  // vector_view<float, 8>
    auto vy = on::simd::view(y);
    auto vz = on::simd::view(z);
    
    // SIMD operations - directly on dp memory
    on::simd::axpy(2.0f, vx, vy);           // y = 2*x + y
    float d = on::simd::dot(vx, vy);        // dot product
    on::simd::copy(vx, vz);                 // z = x
    on::simd::scale(0.5f, vz);              // z *= 0.5
    
    // Results are already in dp containers - no copy needed
    std::cout << "y[0] = " << y[0] << "\n";  // 1.0 (original) + 2*0 = 1.0
    std::cout << "y[1] = " << y[1] << "\n";  // 1.0 + 2*1 = 3.0
    std::cout << "dot  = " << d << "\n";
    
    return 0;
}
```

#### 0.5 Implementation Checklist

##### Step 1: pack<T,W> Infrastructure
- [ ] `simd/pack/pack.hpp` - Primary template (scalar fallback)
- [ ] `simd/pack/pack_sse.hpp` - SSE specializations
  - [ ] `pack<float, 4>`, `pack<double, 2>`
  - [ ] `pack<int32_t, 4>`, `pack<int64_t, 2>`
- [ ] `simd/pack/pack_avx.hpp` - AVX/AVX2 specializations
  - [ ] `pack<float, 8>`, `pack<double, 4>`
  - [ ] `pack<int32_t, 8>`, `pack<int64_t, 4>`
- [ ] `simd/pack/pack_avx512.hpp` - AVX-512 specializations
- [ ] `simd/pack/pack_neon.hpp` - ARM NEON specializations
- [ ] Tests for pack<T,W>

##### Step 2: mask<T,W> and Comparisons
- [ ] `simd/mask.hpp` - mask type + comparison functions
- [ ] Masked load/store
- [ ] blend/select
- [ ] Tests for mask operations

##### Step 3: Kernel and Views
- [ ] `simd/kernel.hpp` - Kernel<T,W,Rank>
- [ ] `simd/view/scalar_view.hpp`
- [ ] `simd/view/vector_view.hpp`
- [ ] `simd/view/matrix_view.hpp`
- [ ] `simd/view/tensor_view.hpp`
- [ ] `simd/view/view.hpp` - module header
- [ ] Tests for views

##### Step 4: Bridge to datapod
- [ ] `simd/bridge.hpp` - view<W>() factory functions
- [ ] Auto-width detection
- [ ] Tests for bridge

##### Step 5: Algorithms
- [ ] `simd/algo/elementwise.hpp` - add, sub, mul, div, axpy, scale, copy
- [ ] `simd/algo/reduce.hpp` - sum, min, max, prod, dot, norm
- [ ] `simd/algo/transform.hpp` - apply, map
- [ ] `simd/algo/algo.hpp` - module header
- [ ] Tests for algorithms

##### Step 6: Update math/ to use pack<T,W>
- [ ] Update `fast_exp.hpp` to use pack<T,W> instead of raw intrinsics
- [ ] Update `fast_log.hpp`
- [ ] Update `fast_trig.hpp`
- [ ] Update `fast_hyp.hpp`
- [ ] Update `fast_pow.hpp`
- [ ] Update `simd_math.hpp`

##### Step 7: Update lina/ to use views
- [ ] Update `lina/basic/matmul.hpp`
- [ ] Update `lina/basic/transpose.hpp`
- [ ] Update `lina/basic/norm.hpp`
- [ ] Update `lina/solve/solve.hpp`
- [ ] Update `lina/solve/lstsq.hpp`
- [ ] Update decomposition algorithms
- [ ] Update tests

##### Step 8: Cleanup
- [ ] Delete old owning types (scalar.hpp, vector.hpp, matrix.hpp, tensor.hpp)
- [ ] Delete old intrinsic/ folder
- [ ] Delete old backend/ folder
- [ ] Update simd.hpp module header
- [ ] Update examples
- [ ] Update documentation

#### 0.6 Migration Notes

**Breaking changes:**
- `simd::Scalar<T>`, `simd::Vector<T,N>`, `simd::Matrix<T,R,C>` are removed
- Use `dp::mat::*` for data ownership
- Use `simd::view<W>(dp_obj)` to get SIMD views
- `SIMDVec<T,W>` renamed to `pack<T,W>`

**lina/ changes:**
- Functions like `lina::matmul(A, B)` will accept either:
  - `dp::mat::matrix` directly (creates views internally), OR
  - `simd::matrix_view` explicitly
- Return types change from `simd::Matrix` to `void` (result passed as out param)
  or return `dp::mat::matrix` directly

---

### Phase 1: SIMD Foundation [DONE]

#### 1.1 Architecture Detection [DONE]
- [x] `simd/arch/arch.hpp` - Platform & SIMD capability detection
  - [x] Compiler detection (GCC, Clang, MSVC, Intel)
  - [x] Platform detection (Windows, Linux, macOS)
  - [x] x86 SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2
  - [x] x86 SIMD: AVX, AVX2, AVX-512 (F, VL, BW, DQ, CD)
  - [x] x86 SIMD: FMA, F16C, BMI1, BMI2
  - [x] ARM SIMD: NEON, SVE, SVE2
  - [x] SIMD width constants
  - [x] Feature query functions: `has_sse()`, `has_avx()`, etc.
- [x] `simd/arch/macros.hpp` - Utility macros
  - [x] `OPTINUM_INLINE` / `OPTINUM_NOINLINE`
  - [x] `OPTINUM_SIMD_ALIGN` / `OPTINUM_SIMD_ALIGNMENT`
  - [x] `OPTINUM_RESTRICT`
  - [x] `OPTINUM_LIKELY` / `OPTINUM_UNLIKELY`
  - [x] `OPTINUM_PREFETCH_READ` / `OPTINUM_PREFETCH_WRITE`
  - [x] `OPTINUM_UNREACHABLE` / `OPTINUM_ASSUME`
  - [x] `OPTINUM_VECTORIZE` / `OPTINUM_UNROLL`

#### 1.2 SIMD Register Abstraction [DONE]
- [x] `simd/intrinsic/simd_vec.hpp` - Main `SIMDVec<T, Width>` template (scalar fallback)
- [x] `simd/intrinsic/sse.hpp` - SSE specializations
  - [x] `SIMDVec<float, 4>` wrapping `__m128`
  - [x] `SIMDVec<double, 2>` wrapping `__m128d`
  - [x] Operations: load, loadu, store, storeu, broadcast
  - [x] Operations: add, sub, mul, div, fma/fms
  - [x] Operations: sqrt, rsqrt, min, max
  - [x] Operations: hsum/hmin/hmax (horizontal reductions)
- [x] `simd/intrinsic/avx.hpp` - AVX specializations
- [x] `simd/intrinsic/avx512.hpp` - AVX-512 specializations
- [x] `simd/intrinsic/neon.hpp` - ARM NEON specializations

#### 1.3 Backend Operations [DONE]
- [x] `simd/backend/backend.hpp` - Common utilities
- [x] `simd/backend/elementwise.hpp` - add, sub, mul, div (+ scalar variants)
- [x] `simd/backend/reduce.hpp` - sum, min, max
- [x] `simd/backend/dot.hpp` - Dot product
- [x] `simd/backend/norm.hpp` - L2 norm, normalize
- [x] `simd/backend/matmul.hpp` - Matrix multiplication + matvec (column-major)
- [x] `simd/backend/transpose.hpp` - Matrix transpose (column-major)

#### 1.4 Update Tensor/Matrix to Use Backend [DONE]
- [x] Update `simd/tensor.hpp` to use backend
- [x] Update `simd/matrix.hpp` to use backend
- [x] Maintain `constexpr` for compile-time evaluation (`std::is_constant_evaluated()` scalar fallback)

#### 1.5 SIMD Math (Fastor-like) [DONE]
- [x] `simd/math/simd_math.hpp` - Module header (public SIMD math API)
- [x] `simd/math/elementary.hpp` - min/max/clamp, abs, sign, ceil/floor/round/trunc
- [x] `simd/math/exponential.hpp` - exp/exp2/expm1, log/log2/log10/log1p
- [x] `simd/math/trig.hpp` - sin/cos/tan, asin/acos/atan/atan2
- [x] `simd/math/hyperbolic.hpp` - sinh/cosh/tanh, asinh/acosh/atanh
- [x] `simd/math/pow.hpp` - pow, cbrt
- [x] `simd/math/special.hpp` - erf/erfc, hypot, tgamma/lgamma

- [x] Policy: always provide scalar fallback (`std::`), SIMD paths use intrinsics and/or poly approximations

#### 1.6 SIMD Coverage Breadth (beyond float/double) [IN PROGRESS]
- [x] Integer SIMD: `SIMDVec<int32_t, 4/8/16>`, `SIMDVec<int64_t, 2/4/8>` (x86)
- [ ] Integer SIMD: NEON equivalents
- [ ] Comparisons and masks: `cmp_eq/ne/lt/le/gt/ge` returning mask + `any/all`, `blend/select`
- [ ] Masked load/store + remainder-safe kernels (optional gather/scatter later)
- [x] Horizontal reductions: added `hprod` for all types
- [x] Bitwise ops for integer SIMD (and/or/xor, shifts, shr_logical)
- [ ] Complex SIMD (TBD): depends on datapod complex type decision; keep API aligned with dp types

### Phase 2: Linear Algebra [DONE]
- [x] `lina/basic/` - matmul, transpose, inverse, determinant
- [x] `lina/decompose/` - LU, QR, SVD, Cholesky, Eigen
- [x] `lina/solve/` - Linear solvers + lstsq
- [x] `lina/algebra/` - einsum, contraction, inner/outer
- [x] `lina/expr/` - Minimal expression templates

### Phase 3: Optimization Core [FUTURE]
- [ ] `opti/core/` - Function traits, interface
- [ ] `opti/callback/` - Callback system
- [ ] `opti/gradient/` - GD, SGD

### Phase 4: Optimizers [FUTURE]
- [ ] `opti/adaptive/` - Adam, RMSProp, etc.
- [ ] `opti/quasi_newton/` - L-BFGS
- [ ] `opti/evolutionary/` - CMA-ES, DE, PSO

### Phase 5: Advanced [FUTURE]
- [ ] `opti/proximal/` - FISTA, Frank-Wolfe
- [ ] `opti/constrained/` - Augmented Lagrangian
- [ ] `opti/multiobjective/` - NSGA-II, MOEA/D

---

## Component Details

### SIMDVec<T, Width> Interface

```cpp
namespace optinum::simd {

template <typename T, std::size_t Width>
class SIMDVec {
public:
    using value_type = T;
    static constexpr std::size_t width = Width;
    
    // Construction
    SIMDVec() = default;
    explicit SIMDVec(T val);                    // Broadcast scalar
    
    // Load/Store (aligned)
    static SIMDVec load(const T* ptr);
    void store(T* ptr) const;
    
    // Load/Store (unaligned)
    static SIMDVec loadu(const T* ptr);
    void storeu(T* ptr) const;
    
    // Arithmetic
    SIMDVec operator+(SIMDVec rhs) const;
    SIMDVec operator-(SIMDVec rhs) const;
    SIMDVec operator*(SIMDVec rhs) const;
    SIMDVec operator/(SIMDVec rhs) const;
    SIMDVec operator-() const;                  // Negation
    
    // FMA (Fused Multiply-Add)
    static SIMDVec fma(SIMDVec a, SIMDVec b, SIMDVec c);  // a*b + c
    static SIMDVec fms(SIMDVec a, SIMDVec b, SIMDVec c);  // a*b - c
    
    // Math
    SIMDVec sqrt() const;
    SIMDVec rsqrt() const;                      // 1/sqrt(x) approximate
    SIMDVec abs() const;
    
    // Min/Max
    static SIMDVec min(SIMDVec a, SIMDVec b);
    static SIMDVec max(SIMDVec a, SIMDVec b);
    
    // Reductions
    T hsum() const;                             // Horizontal sum
    T hmin() const;                             // Horizontal min
    T hmax() const;                             // Horizontal max
    
private:
    native_type data_;  // __m128, __m256, etc.
};

} // namespace optinum::simd
```

### Backend Usage Pattern

```cpp
// simd/backend/elementwise.hpp
namespace optinum::simd::backend {

template<typename T, std::size_t N>
OPTINUM_INLINE void add(T* OPTINUM_RESTRICT dst, 
                        const T* OPTINUM_RESTRICT src1, 
                        const T* OPTINUM_RESTRICT src2) {
    constexpr std::size_t W = arch::simd_width<T>();
    constexpr std::size_t main_loop = (N / W) * W;
    
    // Main SIMD loop
    for (std::size_t i = 0; i < main_loop; i += W) {
        auto a = SIMDVec<T, W>::loadu(src1 + i);
        auto b = SIMDVec<T, W>::loadu(src2 + i);
        (a + b).storeu(dst + i);
    }
    
    // Remainder (scalar)
    for (std::size_t i = main_loop; i < N; ++i) {
        dst[i] = src1[i] + src2[i];
    }
}

} // namespace optinum::simd::backend
```

---

## Testing

**Every header file in `include/` must have a corresponding test file in `test/`.**

```
include/optinum/simd/arch/arch.hpp       ->  test/simd/arch/arch_test.cpp       âœ“
include/optinum/simd/scalar.hpp          ->  test/simd/scalar_test.cpp          âœ“
include/optinum/simd/vector.hpp          ->  test/simd/vector_test.cpp          âœ“
include/optinum/simd/matrix.hpp          ->  test/simd/matrix_test.cpp          âœ“
include/optinum/simd/intrinsic/simd_vec.hpp ->  test/simd/intrinsic/simd_vec_test.cpp âœ“
include/optinum/simd/intrinsic/sse.hpp   ->  test/simd/intrinsic/sse_test.cpp   âœ“
include/optinum/simd/intrinsic/avx.hpp   ->  test/simd/intrinsic/avx_test.cpp   âœ“
include/optinum/simd/intrinsic/avx512.hpp -> test/simd/intrinsic/avx512_test.cpp âœ“
include/optinum/simd/intrinsic/neon.hpp  ->  test/simd/intrinsic/neon_test.cpp  âœ“
include/optinum/simd/backend/elementwise.hpp -> test/simd/backend/elementwise_test.cpp âœ“
include/optinum/simd/backend/reduce.hpp  ->  test/simd/backend/reduce_test.cpp  âœ“
include/optinum/simd/backend/dot.hpp     ->  test/simd/backend/dot_test.cpp     âœ“
include/optinum/simd/backend/norm.hpp    ->  test/simd/backend/norm_test.cpp    âœ“
include/optinum/simd/backend/matmul.hpp  ->  test/simd/backend/matmul_test.cpp  âœ“
include/optinum/simd/backend/transpose.hpp -> test/simd/backend/transpose_test.cpp âœ“
include/optinum/simd/math/simd_math.hpp        ->  test/simd/math/simd_math_test.cpp
include/optinum/simd/math/elementary.hpp       ->  test/simd/math/elementary_test.cpp
include/optinum/simd/math/exponential.hpp      ->  test/simd/math/exponential_test.cpp
include/optinum/simd/math/trig.hpp             ->  test/simd/math/trig_test.cpp
include/optinum/simd/math/hyperbolic.hpp       ->  test/simd/math/hyperbolic_test.cpp
include/optinum/simd/math/pow.hpp              ->  test/simd/math/pow_test.cpp
include/optinum/simd/math/special.hpp          ->  test/simd/math/special_test.cpp

include/optinum/lina/lina.hpp                  ->  test/lina/lina_test.cpp âœ“
include/optinum/lina/basic/matmul.hpp          ->  test/lina/basic/lina_matmul_test.cpp âœ“
include/optinum/lina/basic/transpose.hpp       ->  test/lina/basic/lina_transpose_test.cpp âœ“
include/optinum/lina/basic/determinant.hpp     ->  test/lina/basic/determinant_test.cpp âœ“
include/optinum/lina/basic/inverse.hpp         ->  test/lina/basic/inverse_test.cpp âœ“
include/optinum/lina/basic/norm.hpp            ->  test/lina/basic/lina_norm_test.cpp âœ“
include/optinum/lina/decompose/lu.hpp          ->  test/lina/decompose/lu_test.cpp âœ“
include/optinum/lina/decompose/qr.hpp          ->  test/lina/decompose/qr_test.cpp âœ“
include/optinum/lina/decompose/cholesky.hpp    ->  test/lina/decompose/cholesky_test.cpp âœ“
include/optinum/lina/decompose/eigen.hpp       ->  test/lina/decompose/eigen_test.cpp âœ“
include/optinum/lina/decompose/svd.hpp         ->  test/lina/decompose/svd_test.cpp âœ“
include/optinum/lina/solve/solve.hpp           ->  test/lina/solve/solve_test.cpp âœ“
include/optinum/lina/solve/lstsq.hpp           ->  test/lina/solve/lstsq_test.cpp âœ“
include/optinum/lina/algebra/einsum.hpp        ->  test/lina/algebra/einsum_test.cpp âœ“
include/optinum/lina/algebra/contraction.hpp   ->  test/lina/algebra/contraction_test.cpp âœ“
include/optinum/lina/expr/expr.hpp             ->  test/lina/expr/expr_test.cpp âœ“
include/optinum/opti/problem/sphere.hpp  ->  test/opti/problem/sphere_test.cpp  âœ“
```

Tests use **doctest**. Do NOT add `DOCTEST_CONFIG_IMPLEMENT_WITH_MAIN`.

---

## Design Principles

1. **Header-only**: No compilation, just include
2. **Composition over inheritance**: `on::simd::*` wraps `dp::*` via `.pod()`
3. **Zero-cost abstractions**: Expression templates, compile-time dimensions
4. **SIMD everywhere**: All math operations vectorized when possible
5. **Constexpr friendly**: Scalar fallback for compile-time evaluation
6. **POD-friendly**: Easy serialization via `datapod`
7. **Modern C++**: Requires C++20 (concepts, constexpr, fold expressions)
8. **Use datapod over std**: Prefer `dp::` types over `std::` equivalents

---

## datapod Type Mapping

**Always prefer datapod types over std equivalents:**

| Instead of | Use |
|------------|-----|
| `std::vector<T>` | `dp::Vector<T>` |
| `std::array<T,N>` | `dp::Array<T,N>` |
| `std::string` | `dp::String` |
| `std::optional<T>` | `dp::Optional<T>` |
| `std::variant<Ts...>` | `dp::Variant<Ts...>` |
| `std::pair<K,V>` | `dp::Pair<K,V>` |
| `std::tuple<Ts...>` | `dp::Tuple<Ts...>` |
| `std::unordered_map<K,V>` | `dp::Map<K,V>` |
| `std::unordered_set<T>` | `dp::Set<T>` |
| Exceptions | `dp::Result<T,E>` |

---

## Requirements

- **C++20** or later
- **datapod** library (fetched automatically via CMake/xmake)
- **Optional**: AVX2/AVX-512 for best SIMD performance

---

## Build & Test

```bash
make config    # Configure (preserves cache)
make build     # Build examples and tests
make test      # Run all tests
make clean     # Clean build artifacts
```

---

## License

[TBD]

---

## Fastor Parity Gaps (vs `xtra/Fastor`)

The following items are present in Fastor but not (yet) in Optinum. This list is intentionally scoped to the largest missing surface area beyond â€œFastor uses SLEEF, we use our own SIMD mathâ€.

### 1) Rank-N tensor algebra (beyond rank-1/2)

- **What**: General contractions/einsum and tensor-algebra ops for rank > 2 (including network einsum, strided contraction, outer products on higher ranks, etc.)
- **Where**:
  - Extend einsum beyond rank-1/2: `include/optinum/lina/algebra/einsum.hpp`
  - Add rank-N tensor algebra module(s): `include/optinum/lina/algebra/` (e.g. `einsum_rankn.hpp`, `contraction_rankn.hpp`)
  - Add backend kernels as needed: `include/optinum/simd/backend/` (e.g. `contraction.hpp`, `einsum.hpp`)
  - Add tests mirroring Fastor coverage: `test/lina/algebra/`

### 2) Slicing / view DSL (seq/all/last-style) + richer view test coverage

- **What**: Compile-time and runtime slicing helpers (e.g. `all`, `seq`, `last`) and higher-level view composition similar to Fastorâ€™s â€œviewsâ€.
- **Where**:
  - Add slicing primitives + index/range types: `include/optinum/simd/view/` (new headers)
  - Integrate with `Kernel`/`*_view`: `include/optinum/simd/kernel.hpp`, `include/optinum/simd/view/*.hpp`
  - Add extensive tests: `test/simd/view/` (cover 1D/2D/ND, mixed/overlapping assignments)

### 3) External memory wrapping (`TensorMap` equivalent)

- **What**: Non-owning tensor/matrix/vector adapters that can wrap user memory with explicit extents/strides (Fastorâ€™s `TensorMap` concept).
- **Where**:
  - Add `Map` types (or align naming with existing views): `include/optinum/simd/` and/or `include/optinum/simd/view/`
  - Add construction/interop helpers: `include/optinum/simd/bridge.hpp`
  - Add tests: `test/simd/` and `test/lina/`

### 4) Complex-number support (SIMD + math + linalg)

- **What**: `std::complex<float/double>` support across SIMD vectors/packs, expression templates, and selected linalg kernels/tests.
- **Where**:
  - Add complex pack/intrinsics wrappers: `include/optinum/simd/pack/` and/or `include/optinum/simd/intrinsic/`
  - Add complex-aware ops (dot/norm/matmul/etc.): `include/optinum/simd/backend/` + `include/optinum/lina/`
  - Add tests: `test/simd/*` and `test/lina/*`

### 5) Expression-template op minimisation / compile-time graph optimisation

- **What**: Compile-time rewrite/minimisation for expression graphs (e.g. greedy matrix-chain, flop-reduction strategies) similar to Fastorâ€™s `opmin` meta layer.
- **Where**:
  - Add meta layer: `include/optinum/lina/expr/` (new â€œmeta/opminâ€ headers)
  - Integrate with existing ETs: `include/optinum/lina/expr/expr.hpp`
  - Add tests/benchmarks: `test/lina/expr/`, `examples/`

### 6) Boolean tensor algebra utilities

- **What**: Boolean / predicate utilities and comparisons (e.g. â€œis orthogonalâ€, â€œis uniformâ€, tolerant equality, etc.) with good test coverage.
- **Where**:
  - Add utilities: `include/optinum/lina/basic/` or `include/optinum/lina/algebra/` (new headers)
  - Add tests: `test/lina/basic/` (and/or `test/lina/algebra/`)

### 7) Packaging parity (CMake config + pkg-config) for consumers

- **What**: `find_package(optinum)` config + version files and optionally a `optinum.pc`, similar to Fastorâ€™s installation UX.
- **Where**:
  - Add CMake config template(s): `cmake/optinumConfig.cmake.in` (new)
  - Generate and install config/version + export targets: update `CMakeLists.txt`
  - Add pkg-config template: `cmake/optinum.pc.in` (new) and update `CMakeLists.txt`
