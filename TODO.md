# OPTINUM - Optimization & Numerics Library

> **HEADER-ONLY C++20 LIBRARY** - No compilation required, just `#include <optinum/optinum.hpp>`

---

## Module Status

| Module | Status | Description |
|--------|--------|-------------|
| `simd/` | **âœ… COMPLETE** | SIMD operations, views, pack<T,W>, math functions (40+) |
| `lina/` | **âœ… COMPLETE** | Linear algebra (112 functions, all major decompositions + DARE) |
| `opti/` | **âœ… PHASE 0+0.5 DONE** | 6 optimizers (GD, Momentum, RMSprop, Adam, GN, LM) + optimal control |
| **API** | **âœ… COMPLETE** | Unified optinum:: namespace (85+ functions) |

**Test Status:** 64/64 test suites, 242+ test cases passing âœ…

---

## ğŸ¯ Current Implementation Status

### âœ… COMPLETE - SIMD Module (simd/)

**Core Infrastructure:**
- âœ… `pack<T,W>` with SSE/AVX/AVX-512/NEON support
- âœ… `mask<T,W>` for conditional operations
- âœ… Views: Vector, Matrix, Tensor (non-owning, zero-copy)
- âœ… Slicing: diagonal, filter, random_access
- âœ… **Dynamic size support** - Runtime-sized vectors/matrices

**40+ SIMD Math Functions:**
- âœ… Exponential/Log: exp, log, sqrt, pow, exp2, log2, log10, cbrt, etc.
- âœ… Trigonometric: sin, cos, tan, asin, acos, atan, atan2
- âœ… Hyperbolic: sinh, cosh, tanh, asinh, acosh, atanh
- âœ… Rounding: ceil, floor, round, trunc
- âœ… Utility: abs, clamp, hypot, isnan, isinf
- âœ… Special: erf, tgamma, lgamma

**Algorithms:**
- âœ… Elementwise: add, sub, mul, div, fill, copy, axpy, scale
- âœ… Reductions: sum, min, max, dot, norm
- âœ… Backend: Specialized 2x2/3x3/4x4 kernels (32-243x speedup)

---

### âœ… COMPLETE - Linear Algebra Module (lina/)

**107 Functions Implemented:**

**Basic Operations (20):**
- âœ… matmul, transpose, inverse, determinant, norm, trace
- âœ… adjoint, cofactor, cond, rcond, rank
- âœ… is_symmetric, is_hermitian, is_positive_definite

**Decompositions (5):**
- âœ… LU (with partial pivoting)
- âœ… QR (Householder reflections)
- âœ… SVD (one-sided Jacobi)
- âœ… Cholesky (SPD matrices)
- âœ… Eigendecomposition (symmetric)

**Solvers (4):**
- âœ… solve (Ax = b via LU)
- âœ… lstsq (least squares via QR)
- âœ… dare (Discrete Algebraic Riccati Equation)
- âœ… lqr_gain (LQR feedback gain from DARE solution)

**Advanced (6):**
- âœ… pinv (pseudo-inverse via SVD)
- âœ… null (null space via SVD)
- âœ… orth (orthonormal basis via QR)
- âœ… kron (Kronecker product)
- âœ… permute (tensor permutations)
- âœ… einsum (Einstein summation)

**Calculus/Differentiation:**
- âœ… **jacobian** - Finite-difference Jacobian matrix computation (forward/central)
- âœ… **gradient** - Finite-difference gradient (optimized for scalar functions)
- âœ… **jacobian_error** - Helper for comparing numerical vs analytical Jacobians
- ğŸ”² hessian - Finite-difference Hessian (future)

**All with SIMD acceleration (60-95% SIMD coverage)**

---

### ğŸš§ IN PROGRESS - Optimization Module (opti/)

**âœ… IMPLEMENTED (4 optimizers):**

1. **Vanilla Gradient Descent** (`vanilla_update.hpp`) âœ…
   - Basic gradient descent: `x -= Î± * âˆ‡f(x)`
   - Stateless, simple, benchmark baseline

2. **Momentum** (`momentum_update.hpp`) âœ…
   - Classical momentum (Rumelhart 1986)
   - SIMD-optimized: 2.1x speedup over scalar
   - Supports both fixed and Dynamic sizes

3. **RMSprop** (`rmsprop_update.hpp`) âœ…
   - Adaptive learning rates (Hinton 2012)
   - SIMD-optimized: 5.8x speedup over scalar
   - Supports both fixed and Dynamic sizes

4. **Adam** (`adam_update.hpp`) âœ…
   - Adaptive moment estimation (Kingma & Ba 2014)
   - SIMD-optimized: 3.6x speedup over scalar
   - Supports both fixed and Dynamic sizes
   - Bias correction for moments

**Infrastructure:**
- âœ… `GradientDescent` optimizer template
- âœ… Callback system (`NoCallback`, custom callbacks)
- âœ… Decay policies (`NoDecay`)
- âœ… Function traits and type system
- âœ… Test problems (Sphere function)
- âœ… **Dynamic size support** - All optimizers work with runtime-sized problems

**Performance:**
- âœ… SIMD-accelerated updates (2-6x faster than scalar)
- âœ… Zero-copy views over datapod types
- âœ… Fixed-size: 100% performance (compile-time SIMD)
- âœ… Dynamic-size: ~90% performance (runtime SIMD dispatch)

---

## ğŸ“‹ TODO - Optimization Components to Implement

### **âœ… Phase 0 + Phase 0.5: COMPLETE - Core Infrastructure + Optimal Control**

**Status:** ALL 4 COMPONENTS IMPLEMENTED AND TESTED âœ…

#### âœ… 0a. **Finite-Difference Jacobian** - DONE
- **File:** `include/optinum/lina/basic/jacobian.hpp`
- **Complexity:** â­â­ Medium (~150 lines)
- **Impact:** Core infrastructure for nonlinear least squares
- **Module:** Linear Algebra (calculus operations)
- **Algorithm:** 
  - Forward difference: `J[i,j] = (f_i(x + hÂ·e_j) - f_i(x)) / h`
  - Central difference: `J[i,j] = (f_i(x + hÂ·e_j) - f_i(x - hÂ·e_j)) / (2h)` (more accurate)
- **Functions:**
  ```cpp
  // Compute Jacobian matrix for f: R^n -> R^m
  lina::jacobian(f, x, h=1e-8, central=true) -> Matrix<T, Dynamic, N>
  
  // Optimized gradient for scalar f: R^n -> R
  lina::gradient(f, x, h=1e-8, central=true) -> Vector<T, N>
  ```
- **Source:** Ported from `graphix/src/graphix/factor/nonlinear/nonlinear_factor.cpp::linearize()`
- **âœ… Implemented:** `include/optinum/lina/basic/jacobian.hpp` (210 lines)
- **âœ… Tests:** 15/15 passing - `test/lina/basic/jacobian_test.cpp`
- **âœ… Features:** Forward/central differences, gradient specialization, error checking

#### âœ… 0b. **Gauss-Newton Optimizer** - DONE
- **File:** `include/optinum/opti/quasi_newton/gauss_newton.hpp`
- **Complexity:** â­â­ Medium (~200 lines)
- **Impact:** Fast solver for nonlinear least squares (robotics, vision, SLAM)
- **Module:** Optimization (second-order methods)
- **Algorithm:** 
  ```
  For each iteration:
    1. Compute Jacobian J and residual b = f(x)
    2. Solve: (J^T * J) * delta = -J^T * b  (normal equations)
    3. Update: x += delta
    4. Check convergence
  ```
- **Dependencies:** 
  - Needs `lina::jacobian()` to compute J
  - Needs `lina::matmul()` for J^T * J and J^T * b
  - Needs `lina::solve()` or Cholesky for symmetric system
- **Source:** Ported from `graphix/include/graphix/factor/nonlinear/gauss_newton.hpp`
- **âœ… Implemented:** `include/optinum/opti/quasi_newton/gauss_newton.hpp` (650+ lines)
- **âœ… Tests:** 9/9 passing - `test/opti/quasi_newton/gauss_newton_test.cpp`
- **âœ… Features:** Multiple solvers, line search, convergence criteria, verbose mode
- **âœ… Example:** `examples/gauss_newton_demo.cpp` (curve fitting, circle fitting, Rosenbrock)

#### âœ… 0c. **Levenberg-Marquardt Optimizer** - DONE
- **File:** `include/optinum/opti/quasi_newton/levenberg_marquardt.hpp`
- **Complexity:** â­â­â­ Medium-Hard (~250 lines)
- **Impact:** More robust than Gauss-Newton, industry standard (scipy, ceres)
- **Module:** Optimization (second-order methods)
- **Algorithm:**
  ```
  For each iteration:
    1. Compute J and b = f(x)
    2. Solve: (J^T * J + Î»*I) * delta = -J^T * b  (damped normal equations)
    3. Try step: x_new = x + delta
    4. If error decreased: accept, Î» /= 10 (approach Gauss-Newton)
       Else: reject, Î» *= 10 (approach gradient descent)
    5. Check convergence
  ```
- **Parameters:**
  - `lambda_init = 1e-3` - Initial damping
  - `lambda_factor = 10.0` - Adjustment factor
  - `min_lambda = 1e-7, max_lambda = 1e7` - Bounds
- **Dependencies:** Same as Gauss-Newton + diagonal addition for damping
- **Source:** Ported from `graphix/include/graphix/factor/nonlinear/levenberg_marquardt.hpp`
- **âœ… Implemented:** `include/optinum/opti/quasi_newton/levenberg_marquardt.hpp` (545 lines)
- **âœ… Tests:** 8/8 passing - `test/opti/quasi_newton/levenberg_marquardt_test.cpp`  
- **âœ… Features:** Adaptive damping, robust to poor initialization, handles ill-conditioned problems
- **âœ… Example:** `examples/levenberg_marquardt_demo.cpp` (robustness demo, bundle adjustment)

**âœ… Phase 0 + Phase 0.5 Complete!**
- âœ… All 4 components implemented (Jacobian, Gauss-Newton, Levenberg-Marquardt, DARE)
- âœ… 38/38 test cases passing (15 Jacobian + 9 GN + 8 LM + 6 DARE)
- âœ… Production-ready, ported from graphix + drivekit
- âœ… API exposed in `optinum::` namespace
- âœ… Examples and demos created (GN demo, LM demo)
- âœ… SIMD-accelerated: 70-95% SIMD coverage across all operations

**Impact:**
- âœ… **Second-order methods** (much faster than gradient descent)
  - Convergence: 5-10 iterations (vs 100+ for gradient descent)
  - Ready for robotics, computer vision, SLAM, bundle adjustment
  - Industry-standard algorithms (used in Ceres, g2o, GTSAM)
- âœ… **Optimal control support** (LQR via DARE)
  - Complete LQR controller implementation path
  - SIMD-accelerated: 3-5x faster than scalar loops
  - Ready for real-time control loops (100+ Hz)
  - Supports both fixed-size and Dynamic matrices (M=1)

**Coverage Achievement:**
- âœ… **graphix:** 100% - All optimization needs covered
- âœ… **drivekit:** 100% - All optimization needs covered (including LQR path tracking)

---

### **Tier 1: Essential (Must Have) - 5 optimizers**

#### 1. **Nesterov Momentum (NAG)** - HIGHEST PRIORITY â­â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/nesterov_momentum_update.hpp`
- **Complexity:** â­ Easy (~60 lines)
- **Impact:** O(1/kÂ²) convergence, used in 40% of momentum papers
- **Algorithm:** Lookahead gradient: `v = Î¼v - Î±âˆ‡f(x + Î¼v); x = x + v`
- **Reference:** Nesterov (1983) "A Method Of Solving A Convex Programming Problem"
- **SIMD:** Same pattern as Momentum (already implemented)

#### 2. **AdaGrad** â­â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/adagrad_update.hpp`
- **Complexity:** â­ Easy (~80 lines)
- **Impact:** Foundation for all adaptive methods (6000+ citations)
- **Algorithm:** `G += gÂ²; x -= Î± * g / (âˆšG + Îµ)` (accumulate squared gradients)
- **Reference:** Duchi et al. (2011) "Adaptive Subgradient Methods"
- **SIMD:** Element-wise ops (like RMSprop)

#### 3. **AdaDelta** â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/adadelta_update.hpp`
- **Complexity:** â­ Easy (~100 lines)
- **Impact:** Fixes AdaGrad's monotonic decay, popular in NLP/RNNs
- **Algorithm:** No manual learning rate! Uses RMS of gradients
- **Reference:** Zeiler (2012) "ADADELTA: An Adaptive Learning Rate Method"
- **SIMD:** Same as RMSprop + one extra EMA

#### 4. **AMSGrad** â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/amsgrad_update.hpp`
- **Complexity:** â­ Trivial (~20 lines - just modify Adam!)
- **Impact:** Fixes Adam's convergence issues, proven to converge
- **Algorithm:** `v_hat = max(v_hat, v)` (non-increasing second moment)
- **Reference:** Reddi et al. (2018) "On the Convergence of Adam and Beyond"
- **SIMD:** Change 1 line in Adam implementation!

#### 5. **L-BFGS** â­â­â­â­â­
- **File:** `include/optinum/opti/quasi_newton/lbfgs.hpp`
- **Complexity:** â­â­â­ Hard (~400 lines)
- **Impact:** THE quasi-Newton method, industry standard
- **Algorithm:** Limited-memory BFGS with line search
- **Reference:** Liu & Nocedal (1989) "On the Limited Memory BFGS Method"
- **SIMD:** Vector updates, dot products (60% SIMD coverage)
- **Note:** Requires line search implementation

---

### **Tier 2: Very Important (Should Have) - 3 optimizers**

#### 6. **Lookahead** â­â­â­â­
- **File:** `include/optinum/opti/meta/lookahead.hpp`
- **Complexity:** â­ Easy (~80 lines)
- **Impact:** Meta-optimizer wrapper, improves ANY base optimizer
- **Algorithm:** Slow weights = slow + Î±(fast - slow) every k steps
- **Reference:** Zhang et al. (2019) "Lookahead Optimizer"
- **SIMD:** Trivial (just weight averaging)
- **Usage:** `Lookahead<Adam>`, `Lookahead<SGD>`, etc.

#### 7. **AdaBound / AMSBound** â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/adabound_update.hpp`
- **Complexity:** â­ Easy (~100 lines)
- **Impact:** Transitions from Adam â†’ SGD during training
- **Algorithm:** Adam with bounded learning rates [0.1-Î±, Î±]
- **Reference:** Luo et al. (2019) "Adaptive Gradient Methods with Dynamic Bound"
- **SIMD:** Same as Adam + clipping

#### 8. **Yogi** â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/yogi_update.hpp`
- **Complexity:** â­ Easy (~90 lines)
- **Impact:** Google's Adam improvement, used in production
- **Algorithm:** `v = v - (1-Î²â‚‚) * sign(v - gÂ²) * gÂ²` (gentler decay)
- **Reference:** Zaheer et al. (2018) "Adaptive Methods for Nonconvex Optimization"
- **SIMD:** Same as Adam, just change v update

---

### **Tier 3: Nice to Have (Specialized) - 2 optimizers**

#### 9. **NAdam** â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/nadam_update.hpp`
- **Complexity:** â­ Easy (~90 lines)
- **Impact:** Nesterov + Adam, popular in Keras
- **Algorithm:** Adam with Nesterov momentum
- **Reference:** Dozat (2016) "Incorporating Nesterov Momentum into Adam"
- **SIMD:** Combine Adam + Nesterov patterns

#### 10. **SWATS** â­â­
- **File:** `include/optinum/opti/meta/swats.hpp`
- **Complexity:** â­â­ Medium (~150 lines)
- **Impact:** Auto-switches Adam â†’ SGD
- **Algorithm:** Start with Adam, switch to SGD when stable
- **Reference:** Keskar & Socher (2017) "Improving Generalization Performance"
- **SIMD:** Same as base optimizers + switching logic

---

## ğŸ“Š Implementation Priority Summary

| Rank | Component | Type | Difficulty | Lines | Impact | Priority |
|------|-----------|------|-----------|-------|---------|----------|
| **âœ… Phase 0: COMPLETE (from graphix)** |
| âœ… 0a | **Jacobian** | Lina | â­â­ Medium | 210 | â­â­â­â­â­ | **DONE** |
| âœ… 0b | **Gauss-Newton** | Opti | â­â­ Medium | 650+ | â­â­â­â­â­ | **DONE** |
| âœ… 0c | **Levenberg-Marquardt** | Opti | â­â­â­ Hard | 545 | â­â­â­â­â­ | **DONE** |
| **Tier 1: Essential First-Order** |
| 1 | **Nesterov** | Opti | â­ Easy | ~60 | â­â­â­â­â­ | **MUST** |
| 2 | **AdaGrad** | Opti | â­ Easy | ~80 | â­â­â­â­â­ | **MUST** |
| 3 | **AdaDelta** | Opti | â­ Easy | ~100 | â­â­â­â­ | **MUST** |
| 4 | **AMSGrad** | Opti | â­ Trivial | ~20 | â­â­â­â­ | **MUST** |
| 5 | **L-BFGS** | Opti | â­â­â­ Hard | ~400 | â­â­â­â­â­ | **MUST** |
| **Tier 2: Very Important** |
| 6 | **Lookahead** | Opti | â­ Easy | ~80 | â­â­â­â­ | High |
| 7 | **AdaBound** | Opti | â­ Easy | ~100 | â­â­â­ | High |
| 8 | **Yogi** | Opti | â­ Easy | ~90 | â­â­â­ | Medium |
| **Tier 3: Nice to Have** |
| 9 | **NAdam** | Opti | â­ Easy | ~90 | â­â­â­ | Medium |
| 10 | **SWATS** | Opti | â­â­ Medium | ~150 | â­â­ | Low |

**Total estimated effort:** 
- **âœ… Phase 0:** COMPLETE - Jacobian, Gauss-Newton, Levenberg-Marquardt
- **Tiers 1-3:** 6-8 days (10 optimizers remaining)
- **Remaining:** 6-8 days for Tiers 1-3

---

## ğŸ¯ Recommended Implementation Order

### âœ… Phase 0: Core Infrastructure from Graphix - **COMPLETE!** âœ…
1. âœ… **Jacobian computation** (210 lines, 15 tests passing)
   - Created `lina/basic/jacobian.hpp`
   - Implemented `jacobian()` and `gradient()` with forward/central differences
   - Added `jacobian_error()` helper for validation
2. âœ… **Gauss-Newton optimizer** (650+ lines, 9 tests passing)
   - Created `opti/quasi_newton/gauss_newton.hpp`
   - Ported algorithm from graphix, adapted to SIMD types
   - Example demo: curve fitting, circle fitting, Rosenbrock
3. âœ… **Levenberg-Marquardt optimizer** (545 lines, 8 tests passing)
   - Created `opti/quasi_newton/levenberg_marquardt.hpp`
   - Implemented damped GN with adaptive Î»
   - Example demo: robustness comparison, bundle adjustment

**Phase 0 Success:**
- All 32 tests passing (15 Jacobian + 9 GN + 8 LM)
- Production-ready, ported from proven graphix code
- API fully exposed in `optinum::` namespace
- Comprehensive examples and demos

---

### Phase 1: Quick Wins (1-2 days)
4. **AMSGrad** - 20 lines, modify Adam
5. **Nesterov** - 60 lines, huge impact
6. **AdaGrad** - 80 lines, foundation

### Phase 2: Core Adaptive Methods (2-3 days)
7. **AdaDelta** - 100 lines, popular
8. **NAdam** - 90 lines, Nesterov + Adam
9. **Yogi** - 90 lines, Google variant

### Phase 3: Meta-Optimizers (1-2 days)
10. **Lookahead** - 80 lines, wrapper
11. **AdaBound** - 100 lines, hybrid

### Phase 4: Advanced (3-5 days)
12. **L-BFGS** - 400 lines, quasi-Newton
13. **SWATS** - 150 lines, auto-switching

---

## ğŸ—ï¸ Architecture & Design

### Module Structure

```
include/optinum/lina/
â”œâ”€â”€ lina.hpp                          # Main lina header
â”œâ”€â”€ basic/
â”‚   â”œâ”€â”€ ...                           # âœ… Existing (matmul, det, etc.)
â”‚   â””â”€â”€ jacobian.hpp                  # âœ… DONE: Jacobian & gradient (Phase 0)
â””â”€â”€ ...

include/optinum/opti/
â”œâ”€â”€ opti.hpp                          # Main header (expose all to optinum::)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ types.hpp                     # âœ… Result types, OptimizationResult
â”‚   â”œâ”€â”€ function.hpp                  # âœ… Function traits, concepts
â”‚   â””â”€â”€ callbacks.hpp                 # âœ… Callback system
â”œâ”€â”€ decay/
â”‚   â””â”€â”€ no_decay.hpp                  # âœ… Learning rate decay (none for now)
â”œâ”€â”€ gradient/
â”‚   â”œâ”€â”€ gradient_descent.hpp          # âœ… Main GD optimizer template
â”‚   â””â”€â”€ update_policies/
â”‚       â”œâ”€â”€ vanilla_update.hpp        # âœ… Basic SGD
â”‚       â”œâ”€â”€ momentum_update.hpp       # âœ… Classical momentum
â”‚       â”œâ”€â”€ nesterov_momentum_update.hpp  # ğŸ”² TODO: Nesterov (Tier 1)
â”‚       â”œâ”€â”€ rmsprop_update.hpp        # âœ… RMSprop
â”‚       â”œâ”€â”€ adam_update.hpp           # âœ… Adam
â”‚       â”œâ”€â”€ amsgrad_update.hpp        # ğŸ”² TODO: AMSGrad (Tier 1)
â”‚       â”œâ”€â”€ adagrad_update.hpp        # ğŸ”² TODO: AdaGrad (Tier 1)
â”‚       â”œâ”€â”€ adadelta_update.hpp       # ğŸ”² TODO: AdaDelta (Tier 1)
â”‚       â”œâ”€â”€ nadam_update.hpp          # ğŸ”² TODO: NAdam (Tier 3)
â”‚       â”œâ”€â”€ yogi_update.hpp           # ğŸ”² TODO: Yogi (Tier 2)
â”‚       â””â”€â”€ adabound_update.hpp       # ğŸ”² TODO: AdaBound (Tier 2)
â”œâ”€â”€ meta/
â”‚   â”œâ”€â”€ lookahead.hpp                 # ğŸ”² TODO: Lookahead wrapper (Tier 2)
â”‚   â””â”€â”€ swats.hpp                     # ğŸ”² TODO: SWATS (Tier 3)
â”œâ”€â”€ quasi_newton/                     # âœ… Directory for second-order methods
â”‚   â”œâ”€â”€ gauss_newton.hpp              # âœ… DONE: Phase 0 (from graphix) - 650+ lines
â”‚   â”œâ”€â”€ levenberg_marquardt.hpp       # âœ… DONE: Phase 0 (from graphix) - 545 lines
â”‚   â””â”€â”€ lbfgs.hpp                     # ğŸ”² TODO: L-BFGS (Tier 1)
â””â”€â”€ problem/
    â”œâ”€â”€ sphere.hpp                    # âœ… Test function
    â”œâ”€â”€ rosenbrock.hpp                # ğŸ”² TODO: Classic test
    â”œâ”€â”€ rastrigin.hpp                 # ğŸ”² TODO: Multimodal test
    â””â”€â”€ ackley.hpp                    # ğŸ”² TODO: Multimodal test
```

---

## ğŸ”§ Implementation Guidelines

### For All New Optimizers:

1. **File Location:**
   - Update policies: `opti/gradient/update_policies/`
   - Meta-optimizers: `opti/meta/`
   - Quasi-Newton: `opti/quasi_newton/`

2. **Required Interface (for update policies):**
   ```cpp
   struct MyOptimizer {
       // State variables
       std::vector<double> state;
       
       // Parameters
       double param1 = default_value;
       
       // Constructor
       explicit MyOptimizer(double p1 = default) : param1(p1) {}
       
       // Update function (MUST support both fixed and Dynamic sizes)
       template <typename T, std::size_t N>
       void update(simd::Vector<T, N> &x, T step_size, 
                   const simd::Vector<T, N> &gradient) noexcept {
           const std::size_t n = x.size(); // Get runtime size
           
           // Initialize state on first call
           if (state.size() != n) {
               state.resize(n, T{0});
           }
           
           // SIMD dual path pattern
           if constexpr (N == simd::Dynamic) {
               // Runtime SIMD path
               const std::size_t W = simd::backend::preferred_simd_lanes_runtime<T>();
               constexpr std::size_t pack_width = std::is_same_v<T, double> ? 4 : 8;
               using pack_t = simd::pack<T, pack_width>;
               // ... SIMD loops with runtime bounds ...
           } else {
               // Compile-time SIMD path
               constexpr std::size_t W = simd::backend::preferred_simd_lanes<T, N>();
               using pack_t = simd::pack<T, W>;
               // ... SIMD loops with compile-time bounds ...
           }
       }
       
       // Reset state
       void reset() noexcept { state.clear(); }
       
       // Initialize (called by GradientDescent)
       template <typename T, std::size_t N>
       void initialize(std::size_t n) noexcept { state.clear(); }
   };
   ```

3. **SIMD Requirements:**
   - ALL updates must use SIMD (2-6x speedup expected)
   - Support both fixed-size (compile-time) and Dynamic (runtime) vectors
   - Use `if constexpr (N == Dynamic)` to dispatch between paths
   - Never use `N` directly when it might be Dynamic - use `x.size()`

4. **Testing:**
   - Add test in `test/opti/optimizer_comparison_test.cpp`
   - Test both fixed-size and Dynamic-size vectors
   - Verify convergence on Sphere function
   - Compare results between fixed and Dynamic

5. **API Exposure:**
   - Add to `include/optinum/opti/opti.hpp`
   - Expose in `optinum::` namespace via `include/optinum/optinum.hpp`
   - Example: `using Nesterov = opti::NesterovUpdate;`

---

## ğŸ“ Reference Implementations

**Check ensmallen for algorithms:**
- Location: `./xtra/ensmallen/include/ensmallen_bits/`
- Use for understanding math, NOT copying code (different license)
- Our implementation: SIMD-accelerated, dual compile/runtime paths

**Key Differences from ensmallen:**
- âœ… We use SIMD (2-6x faster)
- âœ… We support both fixed and Dynamic sizes
- âœ… We're header-only (easier to integrate)
- âœ… We use datapod types (cleaner ownership)

---

## ğŸ“ˆ Success Metrics

**When all 10 optimizers are done:**

âœ… **Feature Parity:**
- Same core optimizers as PyTorch/TensorFlow
- All major adaptive methods covered
- Both first-order and quasi-Newton available

âœ… **Performance:**
- 2-6x faster than scalar (via SIMD)
- ~90% performance for Dynamic vs fixed sizes
- Zero-copy views over datapod

âœ… **Flexibility:**
- Works with compile-time AND runtime-sized problems
- Meta-optimizers (Lookahead, SWATS) wrap any base optimizer
- Easy to add custom optimizers

âœ… **Quality:**
- All tests passing (60+ tests)
- Proven convergence on test problems
- Production-ready code

---

## ğŸš€ After Optimization Module Complete

**Immediate Value (Phase 0 + Tier 1):**
After implementing Phase 0 and Tier 1, we'll have:
- âœ… 4 first-order optimizers (Vanilla, Momentum, RMSprop, Adam)
- âœ… 3 second-order optimizers (Gauss-Newton, LM, L-BFGS)
- âœ… 4 more first-order variants (Nesterov, AdaGrad, AdaDelta, AMSGrad)
- âœ… Jacobian/gradient computation infrastructure

**This is production-ready for 90% of use cases!**

---

**Future Expansion:**

### **âœ… Phase 0.5: COMPLETE - Controls & Optimal Control**

**Status:** ALL COMPONENTS IMPLEMENTED AND TESTED âœ…

#### âœ… **DARE Solver** - DONE â­â­â­â­â­
- **File:** `include/optinum/lina/solve/dare.hpp`
- **Complexity:** â­â­ Medium (272 lines)
- **Impact:** Completes drivekit LQR controller support
- **Algorithm:** Discrete Algebraic Riccati Equation
  ```
  Solve: P = Q + A^T*P*A - A^T*P*B*(R+B^T*P*B)^{-1}*B^T*P*A
  ```
- **Implementation:** Iterative fixed-point method (ported from drivekit)
- **Dependencies:** matmul, transpose, inverse (all available in optinum)
- **âœ… Implemented:** `include/optinum/lina/solve/dare.hpp` (270 lines - SIMD optimized)
- **âœ… Tests:** 6/6 passing, 38 assertions - `test/lina/solve/dare_test.cpp`
- **âœ… Features:** Fixed-point iteration, convergence detection, scalar control optimization (M=1)
- **âœ… SIMD Acceleration:** 85% SIMD coverage (matmul, transpose, add, subtract, norm_fro)
  - Matrix operations: 95%+ SIMD (via lina::matmul, lina::transpose)
  - Element-wise ops: 100% SIMD (operator+, operator-, norm_fro)
  - Performance: 3-5x faster than scalar loops (typical 4x4 LQR problem)
- **Note:** Dynamic matrices supported with limitation - M>1 requires fixed-size due to inverse() constraints
- **âœ… Functions:**
  ```cpp
  // Solve DARE: P = A^T*P*A - A^T*P*B*(R+B^T*P*B)^{-1}*B^T*P*A + Q
  lina::dare(A, B, Q, R, max_iter=150, tol=1e-6) -> Matrix<T, N, N>
  
  // Compute LQR gain: K = (R+B^T*P*B)^{-1}*B^T*P*A
  lina::lqr_gain(A, B, R, P) -> Matrix<T, M, N>
  ```
- **âœ… API Exposed:** Available in `optinum::` namespace

**âœ… Phase 0.5 Complete!**
- âœ… DARE solver implemented and tested
- âœ… LQR gain computation helper
- âœ… 5/5 new tests passing (2x2, 4x4, identity, M>1 cases)
- âœ… Optinum now supports 100% of graphix + drivekit optimization needs!

**Coverage Achievement:**
- âœ… graphix: 100% coverage (Jacobian, GN, LM)
- âœ… drivekit: 100% coverage (DARE for LQR controller)

---

### **Phase 1: Metaheuristic Module (`meta/`) - NEW CAPABILITY CLASS** ğŸ†•

**Rationale:** Monte Carlo methods (MPPI, CEM) belong to broader "metaheuristic" family:
- Not gradient-based (no derivatives)
- Approximate/stochastic optimization
- Derivative-free global search
- Includes: sampling-based, swarm intelligence, evolutionary, local search

**See:** `METAHEURISTIC_CATEGORIZATION.md` for full taxonomy and design

#### Directory Structure:
```
include/optinum/meta/
â”œâ”€â”€ meta.hpp                       # Main header
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ population.hpp             # Population management
â”‚   â”œâ”€â”€ sampler.hpp                # Base sampler interface
â”‚   â””â”€â”€ selector.hpp               # Selection strategies
â”œâ”€â”€ samplers/
â”‚   â”œâ”€â”€ gaussian_sampler.hpp       # Gaussian noise sampling
â”‚   â”œâ”€â”€ uniform_sampler.hpp        # Uniform random sampling
â”‚   â””â”€â”€ cauchy_sampler.hpp         # Cauchy distribution
â”œâ”€â”€ methods/
â”‚   â”œâ”€â”€ sampling/
â”‚   â”‚   â”œâ”€â”€ mppi.hpp               # Model Predictive Path Integral (drivekit!)
â”‚   â”‚   â”œâ”€â”€ cem.hpp                # Cross-Entropy Method
â”‚   â”‚   â””â”€â”€ monte_carlo.hpp        # Generic Monte Carlo
â”‚   â”œâ”€â”€ swarm/
â”‚   â”‚   â”œâ”€â”€ particle_swarm.hpp     # PSO
â”‚   â”‚   â””â”€â”€ ant_colony.hpp         # ACO
â”‚   â”œâ”€â”€ evolutionary/
â”‚   â”‚   â”œâ”€â”€ genetic_algorithm.hpp  # GA
â”‚   â”‚   â”œâ”€â”€ cma_es.hpp             # CMA-ES
â”‚   â”‚   â””â”€â”€ differential_evolution.hpp # DE
â”‚   â””â”€â”€ local_search/
â”‚       â”œâ”€â”€ simulated_annealing.hpp # SA
â”‚       â””â”€â”€ tabu_search.hpp         # Tabu
â””â”€â”€ aggregators/
    â”œâ”€â”€ softmax_aggregator.hpp     # Exponential weighting (MPPI)
    â”œâ”€â”€ elite_aggregator.hpp       # Top-k selection (CEM)
    â””â”€â”€ tournament_aggregator.hpp  # Tournament selection (GA)
```

#### Priority Methods:

| # | Method | File | Effort | Priority | Why |
|---|--------|------|--------|----------|-----|
| 1 | **MPPI** | `meta/methods/sampling/mppi.hpp` | 4h | â­â­â­â­â­ | Used in drivekit |
| 2 | **PSO** | `meta/methods/swarm/particle_swarm.hpp` | 4h | â­â­â­â­â­ | Very popular |
| 3 | **CEM** | `meta/methods/sampling/cem.hpp` | 3h | â­â­â­â­ | Complements MPPI |
| 4 | **SA** | `meta/methods/local_search/simulated_annealing.hpp` | 3h | â­â­â­â­ | Classic baseline |
| 5 | **GA** | `meta/methods/evolutionary/genetic_algorithm.hpp` | 6h | â­â­â­â­ | Foundation for others |
| 6 | **CMA-ES** | `meta/methods/evolutionary/cma_es.hpp` | 8h | â­â­â­â­â­ | State-of-the-art |
| 7 | **DE** | `meta/methods/evolutionary/differential_evolution.hpp` | 4h | â­â­â­â­ | Simple & effective |

**Total for basic meta module:** ~32 hours (~4 days)

**API Exposure:**
```cpp
namespace optinum {
    // Metaheuristic methods
    using meta::GaussianSampler;
    using meta::SoftmaxAggregator;
    
    template <typename T = double> using MPPI = meta::MPPI<T>;
    template <typename T = double> using CrossEntropy = meta::CrossEntropy<T>;
    template <typename T = double> using ParticleSwarm = meta::ParticleSwarm<T>;
    template <typename T = double> using GeneticAlgorithm = meta::GeneticAlgorithm<T>;
    template <typename T = double> using SimulatedAnnealing = meta::SimulatedAnnealing<T>;
}
```

**After meta module:** Optinum will have gradient-based, quasi-Newton, AND metaheuristic methods!

---

### **Future Expansion (Lower Priority):**
- [ ] More test problems (Rosenbrock, Rastrigin, Ackley)
- [ ] Learning rate schedulers (cosine annealing, step decay)
- [ ] Gradient clipping callbacks
- [ ] Line search algorithms (Armijo, Wolfe) - needed for L-BFGS
- [ ] Stochastic methods (SVRG, SARAH)
- [ ] Constrained optimization (Augmented Lagrangian)

**Not Immediate Priority:**
- These are nice-to-have but not needed for MVP
- Focus on: DARE â†’ meta module â†’ Tier 1 gradient methods
- Can add later based on user demand

---

## ğŸ“š Dependencies

- **C++20** or later
- **datapod** v0.0.10 (fetched automatically)
- **doctest** (for tests, fetched automatically)
- **Optional:** AVX2/AVX-512 for maximum SIMD performance

---

## ğŸ”— References

**Optimization:**
- ensmallen: https://github.com/mlpack/ensmallen
- PyTorch optimizers: https://pytorch.org/docs/stable/optim.html
- Adam paper: Kingma & Ba (2014) https://arxiv.org/abs/1412.6980
- L-BFGS: Liu & Nocedal (1989)

**Metaheuristics:**
- **See:** `METAHEURISTIC_CATEGORIZATION.md` for complete taxonomy
- Blum & Roli (2003): "Metaheuristics in combinatorial optimization"
- Glover (1986): Original "metaheuristic" paper
- Wikipedia Metaheuristic article (comprehensive overview)

**Controls & SLAM:**
- **See:** `GRAPHIX_DRIVEKIT_ANALYSIS.md` for graphix/drivekit requirements
- DARE: Discrete Algebraic Riccati Equation
- LQR: Linear Quadratic Regulator

**SIMD:**
- Intel Intrinsics Guide: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/
- SLEEF: https://sleef.org/

**Testing:**
- Test Problems: https://www.sfu.ca/~ssurjano/optimization.html

---

## ğŸ“ Notes

- All modules follow the same dual-path SIMD pattern (compile-time + runtime)
- This TODO tracks ONLY what's left to do (not what's done)
- See git history for full development timeline
- Current focus: Implementing the 10 priority optimizers

---

## ğŸ“¦ Summary: Graphix Integration (Phase 0)

**What we're porting from `../graphix`:**

1. **Jacobian computation** â†’ `lina/basic/jacobian.hpp`
   - Finite-difference Jacobian for vector functions
   - Optimized gradient for scalar functions
   - Both forward and central differences

2. **Gauss-Newton optimizer** â†’ `opti/quasi_newton/gauss_newton.hpp`
   - Nonlinear least squares solver
   - Fast convergence (5-10 iterations typical)
   - Production-ready (used in graphix SLAM)

3. **Levenberg-Marquardt** â†’ `opti/quasi_newton/levenberg_marquardt.hpp`
   - Damped Gauss-Newton (more robust)
   - Adaptive trust region (adjusts Î»)
   - Industry standard (scipy, ceres, g2o)

**Why these 3?**
- Already proven in production (graphix)
- Fill critical gap (second-order methods)
- Needed for robotics, vision, SLAM applications
- Complement our first-order optimizers

**Total effort:** ~1.5-2 days (12-16 hours)

**After Phase 0:** Optinum will have both first-order (GD, Adam) AND second-order (GN, LM) optimizers!

---

---

## ğŸ‰ PHASE 0 MILESTONE ACHIEVED - December 27, 2025

**Major Achievement:** Second-order optimization methods now available!

**What's New:**
- âœ… **3 new components** - Jacobian, Gauss-Newton, Levenberg-Marquardt
- âœ… **32 new tests** - All passing (15 + 9 + 8)
- âœ… **1,405 lines** of production code
- âœ… **2 example demos** - gauss_newton_demo.cpp, levenberg_marquardt_demo.cpp
- âœ… **Full API exposure** - Available via `optinum::jacobian`, `optinum::GaussNewton<>`, `optinum::LevenbergMarquardt<>`

**Performance:**
- Gauss-Newton: 5-10 iterations typical (vs 100+ for gradient descent)
- Levenberg-Marquardt: Robust to poor initialization
- SIMD-accelerated Jacobian computation
- Production-ready for robotics, vision, SLAM

**Test Status:** 71/71 tests passing âœ… (100% pass rate)

---

---

## ğŸ“‹ Implementation Priority Queue

### **NOW (Critical Path):**
1. â­ï¸ **DARE Solver** (2-3 hours) - Blocks drivekit LQR support
2. ğŸ”œ **Metaheuristic Core** (8 hours) - Samplers, aggregators, infrastructure
3. ğŸ”œ **MPPI** (4 hours) - First metaheuristic method (drivekit-proven)

### **Next (High Value):**
4. PSO, CEM, Simulated Annealing (~10 hours)
5. Tier 1 gradient methods (Nesterov, AdaGrad, AdaDelta, AMSGrad) (~6 hours)
6. L-BFGS (~12 hours)

### **Later (Nice to Have):**
7. Advanced metaheuristics (GA, CMA-ES, DE) (~18 hours)
8. Constrained optimization, line search, schedulers

---

**Last Updated:** December 27, 2025 - Phase 0 Complete! Next: DARE + meta module

---

## ğŸ”¬ Phase 0.7: Lie Groups (Manifold Optimization) - APPROVED

**Status:** âœ… APPROVED - Ready for Implementation

**Prerequisites:** âœ… COMPLETE - Quaternion SIMD Infrastructure
- `simd/pack/quaternion.hpp` - Low-level SIMD pack for quaternions (SoA storage)
- `simd/view/quaternion_view.hpp` - Transparent SIMD view over quaternion arrays **NEW**
- `simd/quaternion.hpp` - Owning container (simplified, uses view internally) **UPDATED**
- `simd/bridge.hpp` - `view()` overloads for automatic SIMD dispatch **UPDATED**
- All tested: 19 quaternion_view tests + 20 pack tests passing

**Location:** `include/optinum/lie/` (new top-level module)

**Rationale:** Critical for proper rotation/pose optimization in graphix (SLAM, bundle adjustment, IMU preintegration, robot kinematics)

**Design Decisions:**
- âœ… New `lie/` module at same level as `simd/`, `lina/`, `opti/`
- âœ… Always use SIMD (leverage existing `simd::quaternion` and math functions)
- âœ… Support batched operations (process N rotations in parallel)
- âœ… Implementation order: SO2 â†’ SE2 â†’ SO3 â†’ SE3
- âœ… All Sophus functions included (not API-compatible, but feature-complete)

---

### Why We Need Lie Groups

Without Lie groups, optimizing rotations is problematic:
- **Rotation matrices (R^9)**: Overparameterized, constraints hard to maintain
- **Euler angles (R^3)**: Gimbal lock, singularities at 2Ï€
- **Quaternions (R^4)**: Need normalization constraint, optimization drifts

**With Lie groups:**
- Natural parameterization (R^1 for SO2, R^3 for SO3)
- No constraints in optimization
- Proper exp/log maps for manifold optimization
- Clean derivatives for Gauss-Newton/Levenberg-Marquardt

---

### Module Structure

```
include/optinum/lie/
â”œâ”€â”€ lie.hpp                      # Main header (includes all)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ constants.hpp            # Epsilon, pi, tolerances (~50 lines)
â”‚   â”œâ”€â”€ concepts.hpp             # C++20 LieGroup concept (~80 lines)
â”‚   â””â”€â”€ rotation_matrix.hpp      # isOrthogonal, makeRotationMatrix (~100 lines)
â”œâ”€â”€ groups/
â”‚   â”œâ”€â”€ so2.hpp                  # SO(2) - 2D rotations (~400 lines)
â”‚   â”œâ”€â”€ se2.hpp                  # SE(2) - 2D rigid transforms (~500 lines)
â”‚   â”œâ”€â”€ so3.hpp                  # SO(3) - 3D rotations (~600 lines)
â”‚   â”œâ”€â”€ se3.hpp                  # SE(3) - 3D rigid transforms (~700 lines)
â”‚   â”œâ”€â”€ rxso2.hpp                # R+ x SO(2) - 2D rotation + scale (~400 lines)
â”‚   â”œâ”€â”€ rxso3.hpp                # R+ x SO(3) - 3D rotation + scale (~500 lines)
â”‚   â”œâ”€â”€ sim2.hpp                 # Sim(2) - 2D similarity (~500 lines)
â”‚   â””â”€â”€ sim3.hpp                 # Sim(3) - 3D similarity (~600 lines)
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ interpolate.hpp          # Lie group interpolation (~100 lines)
â”‚   â”œâ”€â”€ average.hpp              # Biinvariant mean computation (~200 lines)
â”‚   â”œâ”€â”€ spline.hpp               # Lie group splines (~300 lines)
â”‚   â””â”€â”€ geometry.hpp             # Pose/plane/line utilities (~150 lines)
â””â”€â”€ batch/
    â”œâ”€â”€ so3_batch.hpp            # Batched SO3 (N rotations, SIMD) (~300 lines)
    â””â”€â”€ se3_batch.hpp            # Batched SE3 (N poses, SIMD) (~400 lines)

test/lie/
â”œâ”€â”€ so2_test.cpp                 # 15-20 test cases
â”œâ”€â”€ se2_test.cpp                 # 15-20 test cases
â”œâ”€â”€ so3_test.cpp                 # 20-25 test cases
â”œâ”€â”€ se3_test.cpp                 # 20-25 test cases
â”œâ”€â”€ rxso2_test.cpp               # 10-15 test cases
â”œâ”€â”€ rxso3_test.cpp               # 10-15 test cases
â”œâ”€â”€ sim2_test.cpp                # 10-15 test cases
â”œâ”€â”€ sim3_test.cpp                # 10-15 test cases
â”œâ”€â”€ interpolate_test.cpp         # 10-15 test cases
â”œâ”€â”€ average_test.cpp             # 10-15 test cases
â””â”€â”€ batch_test.cpp               # 15-20 test cases

examples/
â”œâ”€â”€ lie_groups_demo.cpp          # Basic Lie group operations
â”œâ”€â”€ rotation_optimization.cpp    # SO3 optimization example
â””â”€â”€ pose_graph_demo.cpp          # SE3 pose graph example
```

**Total Estimate:** ~4,500 lines code + 1,500 lines tests = 6,000 lines

---

### Implementation Phases

#### Phase 0.7a: SO(2) - 2D Rotations â­â­â­â­â­
- **File:** `include/optinum/lie/groups/so2.hpp`
- **Complexity:** â­ Easy (~400 lines)
- **Storage:** Unit complex number (cos Î¸, sin Î¸) as `Vector<T, 2>`
- **DoF:** 1 (rotation angle)
- **Parameters:** 2 (complex number)

**Functions to implement:**
| Category | Function | Description |
|----------|----------|-------------|
| **Core** | `exp(Î¸)` | Tangent â†’ Group: `(cos Î¸, sin Î¸)` |
| | `log()` | Group â†’ Tangent: `atan2(y, x)` |
| | `inverse()` | `(x, -y)` (complex conjugate) |
| | `operator*` | Complex multiplication |
| | `operator*(point)` | Rotate 2D point |
| | `matrix()` | Return 2Ã—2 rotation matrix |
| | `normalize()` | Ensure unit length |
| **Lie Algebra** | `hat(Î¸)` | Î¸ â†’ skew-symmetric 2Ã—2 |
| | `vee(Î©)` | skew-symmetric â†’ Î¸ |
| | `Adj()` | Adjoint (= 1 for SO2) |
| | `lieBracket(a, b)` | [a, b] = 0 (commutative) |
| | `generator()` | Infinitesimal generator |
| **Derivatives** | `Dx_exp_x(Î¸)` | d(exp)/dx |
| | `Dx_exp_x_at_0()` | d(exp)/dx at x=0 |
| | `Dx_this_mul_exp_x_at_0()` | d(this * exp(x))/dx at x=0 |
| | `Dx_log_this_inv_by_x_at_this()` | d(log(thisâ»Â¹ * x))/dx at x=this |
| **Construction** | `SO2()` | Identity |
| | `SO2(Î¸)` | From angle |
| | `SO2(real, imag)` | From complex parts |
| | `SO2(Matrix2)` | From rotation matrix |
| | `fitToSO2(Matrix2)` | Closest SO2 to arbitrary matrix |
| | `sampleUniform(rng)` | Random rotation |
| **Access** | `unit_complex()` | Get (cos, sin) |
| | `data()` | Raw pointer |
| | `params()` | Internal parameters |
| | `cast<NewScalar>()` | Type conversion |

---

#### Phase 0.7b: SE(2) - 2D Rigid Transforms â­â­â­â­
- **File:** `include/optinum/lie/groups/se2.hpp`
- **Complexity:** â­â­ Medium (~500 lines)
- **Storage:** SO2 + Vector2 (rotation + translation)
- **DoF:** 3 (2 translation + 1 rotation)
- **Parameters:** 4 (2 complex + 2 translation)
- **Tangent:** `[vx, vy, Î¸]` (translation first, rotation last)

**Functions to implement:**
| Category | Function | Description |
|----------|----------|-------------|
| **Core** | `exp(twist)` | RÂ³ â†’ SE(2) with left Jacobian |
| | `log()` | SE(2) â†’ RÂ³ twist |
| | `inverse()` | `(Râ»Â¹, -Râ»Â¹ * t)` |
| | `operator*` | Composition: `(R1*R2, t1 + R1*t2)` |
| | `operator*(point)` | Transform 2D point |
| | `operator*(line)` | Transform parametrized line |
| | `operator*(plane)` | Transform hyperplane |
| | `matrix()` | Return 3Ã—3 homogeneous matrix |
| | `matrix2x3()` | Return 2Ã—3 compact form |
| **Lie Algebra** | `hat(twist)` | RÂ³ â†’ se(2) 3Ã—3 matrix |
| | `vee(Î©)` | se(2) â†’ RÂ³ |
| | `Adj()` | 3Ã—3 Adjoint matrix |
| | `lieBracket(a, b)` | se(2) bracket |
| | `generator(i)` | i-th infinitesimal generator |
| **Derivatives** | `Dx_exp_x(twist)` | 4Ã—3 Jacobian |
| | `Dx_exp_x_at_0()` | Jacobian at identity |
| | `Dx_this_mul_exp_x_at_0()` | |
| | `Dx_log_this_inv_by_x_at_this()` | |
| **Construction** | `SE2()` | Identity |
| | `SE2(SO2, Vector2)` | From rotation + translation |
| | `SE2(Î¸, Vector2)` | From angle + translation |
| | `SE2(Matrix3)` | From homogeneous matrix |
| | `rot(Î¸)` | Pure rotation |
| | `trans(x, y)` | Pure translation |
| | `transX(x)`, `transY(y)` | Axis translations |
| | `fitToSE2(Matrix3)` | Closest SE2 |
| | `sampleUniform(rng)` | Random pose |
| **Access** | `so2()` | Rotation component |
| | `translation()` | Translation component |
| | `rotationMatrix()` | 2Ã—2 rotation matrix |
| | `setComplex()`, `setRotationMatrix()` | Mutators |

---

#### Phase 0.7c: SO(3) - 3D Rotations â­â­â­â­â­
- **File:** `include/optinum/lie/groups/so3.hpp`
- **Complexity:** â­â­â­ Hard (~600 lines)
- **Storage:** `dp::mat::quaternion<T>` with `[w, x, y, z]` convention (scalar first)
- **DoF:** 3 (rotation vector / axis-angle)
- **Parameters:** 4 (quaternion)
- **SIMD:** Uses `simd::quaternion_view` for batched ops, `pack<quaternion>` internally
- **Note:** Storage is `dp::mat::quaternion<T>`, enabling implicit conversion to/from `dp::Quaternion`

**Functions to implement:**
| Category | Function | Description |
|----------|----------|-------------|
| **Core** | `exp(Ï‰)` | RÂ³ â†’ SO(3) via quaternion: `q = [sin(Î¸/2)*Ï‰Ì‚, cos(Î¸/2)]` |
| | `expAndTheta(Ï‰, &Î¸)` | exp + return angle (reuse in Jacobian) |
| | `log()` | SO(3) â†’ RÂ³: `2 * atan2(|v|, w) * v/|v|` |
| | `logAndTheta()` | log + return angle |
| | `inverse()` | Quaternion conjugate |
| | `operator*` | Hamilton product |
| | `operator*(point)` | Rotate 3D point: `q * v * qâ»Â¹` |
| | `operator*(line)` | Rotate parametrized line |
| | `operator*(plane)` | Rotate hyperplane |
| | `matrix()` | Return 3Ã—3 rotation matrix |
| | `normalize()` | Ensure unit quaternion |
| **Lie Algebra** | `hat(Ï‰)` | RÂ³ â†’ so(3) skew-symmetric 3Ã—3 |
| | `vee(Î©)` | so(3) â†’ RÂ³ |
| | `Adj()` | = rotation matrix |
| | `lieBracket(a, b)` | = cross product `a Ã— b` |
| | `generator(i)` | i-th infinitesimal generator |
| **Jacobians** | `leftJacobian(Ï‰)` | J_l(Ï‰) 3Ã—3 matrix |
| | `leftJacobianInverse(Ï‰)` | J_lâ»Â¹(Ï‰) |
| | `Dx_exp_x(Ï‰)` | 4Ã—3 derivative of exp |
| | `Dx_exp_x_at_0()` | Jacobian at identity |
| | `Dx_this_mul_exp_x_at_0()` | 4Ã—3 |
| | `Dx_log_this_inv_by_x_at_this()` | 3Ã—4 |
| | `Dx_exp_x_times_point_at_0(p)` | 3Ã—3 |
| **Construction** | `SO3()` | Identity |
| | `SO3(Quaternion)` | From quaternion (normalizes) |
| | `SO3(Matrix3)` | From rotation matrix |
| | `rotX(Î¸)`, `rotY(Î¸)`, `rotZ(Î¸)` | Axis rotations |
| | `fitToSO3(Matrix3)` | Closest SO3 via SVD |
| | `sampleUniform(rng)` | Uniform on sphere |
| **Access** | `unit_quaternion()` | Get quaternion |
| | `angleX()`, `angleY()`, `angleZ()` | Extract Euler angles |
| | `data()`, `params()`, `cast<>()` | Standard accessors |

**Key Formulas:**
```
Exp: q = [sin(Î¸/2) * Ï‰/|Ï‰|, cos(Î¸/2)]  where Î¸ = |Ï‰|
Log: Ï‰ = 2 * atan2(|v|, w) * v/|v|  (Taylor series for small angles)
Left Jacobian: J_l(Ï‰) = I + (1-cos Î¸)/Î¸Â² [Ï‰]Ã— + (Î¸-sin Î¸)/Î¸Â³ [Ï‰]Ã—Â²
```

---

#### Phase 0.7d: SE(3) - 3D Rigid Transforms â­â­â­â­â­
- **File:** `include/optinum/lie/groups/se3.hpp`
- **Complexity:** â­â­â­ Hard (~700 lines)
- **Storage:** SO3 + Vector3 (quaternion + translation)
- **DoF:** 6 (3 translation + 3 rotation)
- **Parameters:** 7 (4 quaternion + 3 translation)
- **Tangent:** `[vx, vy, vz, Ï‰x, Ï‰y, Ï‰z]` (translation first, rotation last)

**Functions to implement:**
| Category | Function | Description |
|----------|----------|-------------|
| **Core** | `exp(twist)` | Râ¶ â†’ SE(3): `T = [R, V*Ï…]` where V = J_l(Ï‰) |
| | `log()` | SE(3) â†’ Râ¶: `[Vâ»Â¹*t, Ï‰]` |
| | `inverse()` | `(Râ»Â¹, -Râ»Â¹ * t)` |
| | `operator*` | `(R1*R2, t1 + R1*t2)` |
| | `operator*(point)` | `R*p + t` |
| | `operator*(line)` | Transform line |
| | `operator*(plane)` | Transform plane |
| | `matrix()` | 4Ã—4 homogeneous |
| | `matrix3x4()` | 3Ã—4 compact form |
| **Lie Algebra** | `hat(twist)` | Râ¶ â†’ se(3) 4Ã—4 |
| | `vee(Î©)` | se(3) â†’ Râ¶ |
| | `Adj()` | 6Ã—6 Adjoint: `[[R, [t]Ã—R], [0, R]]` |
| | `lieBracket(a, b)` | se(3) bracket |
| | `generator(i)` | i-th generator |
| **Jacobians** | `leftJacobian(twist)` | 6Ã—6 matrix |
| | `leftJacobianInverse(twist)` | 6Ã—6 |
| | `Dx_exp_x(twist)` | 7Ã—6 |
| | `Dx_exp_x_at_0()` | 7Ã—6 at identity |
| | `Dx_this_mul_exp_x_at_0()` | 7Ã—6 |
| | `Dx_log_this_inv_by_x_at_this()` | 6Ã—7 |
| **Construction** | `SE3()` | Identity |
| | `SE3(SO3, Vector3)` | From rotation + translation |
| | `SE3(Quaternion, Vector3)` | From quat + translation |
| | `SE3(Matrix3, Vector3)` | From R + t |
| | `SE3(Matrix4)` | From homogeneous matrix |
| | `rotX/Y/Z(Î¸)` | Pure rotations |
| | `trans(x,y,z)` | Pure translation |
| | `transX/Y/Z(d)` | Axis translations |
| | `fitToSE3(Matrix4)` | Closest SE3 |
| | `sampleUniform(rng)` | Random pose |
| **Access** | `so3()` | Rotation component |
| | `translation()` | Translation component |
| | `rotationMatrix()` | 3Ã—3 matrix |
| | `unit_quaternion()` | Get quaternion |

---

#### Phase 0.7e: Similarity Groups (RxSO2, RxSO3, Sim2, Sim3) â­â­â­
- **Files:** `rxso2.hpp`, `rxso3.hpp`, `sim2.hpp`, `sim3.hpp`
- **Complexity:** â­â­ Medium (~400-600 lines each)
- **Purpose:** Rotation + scaling (for scale-invariant problems)

**RxSO2/RxSO3** (Rotation + Scale):
- Storage: Non-unit complex/quaternion (normÂ² = scale)
- DoF: 2 (RxSO2), 4 (RxSO3)
- `scale()` - extract scale factor
- `so2()`/`so3()` - extract rotation only

**Sim2/Sim3** (Similarity = RxSO + Translation):
- Storage: RxSO + translation
- DoF: 4 (Sim2), 7 (Sim3)
- Useful for: monocular SLAM, loop closure with scale drift

---

#### Phase 0.7f: Algorithms â­â­â­â­
- **Files:** `algorithms/*.hpp`

**interpolate.hpp:**
```cpp
// Geodesic interpolation: exp(t * log(aâ»Â¹ * b)) * a
template <class G>
G interpolate(const G& a, const G& b, Scalar t);
```

**average.hpp:**
```cpp
// Biinvariant mean (iterative or closed-form)
template <class Container>
std::optional<G> average(const Container& poses);

template <class Container>
std::optional<G> iterativeMean(const Container& poses, int max_iter = 20);
```

**spline.hpp:**
```cpp
// Lie group splines for smooth trajectories
template <class G>
class LieSpline { ... };
```

**geometry.hpp:**
```cpp
// Construct rotation from normal vector
SO2<T> SO2FromNormal(Vector2<T> normal);
SO3<T> SO3FromNormal(Vector3<T> normal);

// Line/plane from pose
Line2<T> lineFromSE2(SE2<T> pose);
Plane3<T> planeFromSE3(SE3<T> pose);

// Pose from line/plane
SE2<T> SE2FromLine(Line2<T> line);
SE3<T> SE3FromPlane(Plane3<T> plane);
```

---

#### Phase 0.7g: Batched Operations (SIMD) â­â­â­â­â­
- **Files:** `batch/so3_batch.hpp`, `batch/se3_batch.hpp`
- **Purpose:** Process N rotations/poses in parallel using SIMD
- **Status:** âœ… FOUNDATION READY - Uses `simd::quaternion_view` infrastructure

**Implementation Strategy:**

The batched Lie group operations leverage the new transparent SIMD quaternion infrastructure:

```cpp
// === EXISTING INFRASTRUCTURE (simd/) ===

// 1. quaternion_view - transparent SIMD over dp::mat::quaternion arrays
#include <optinum/simd/view/quaternion_view.hpp>

dp::mat::quaternion<double> quats[8];
auto qv = simd::view(quats);        // auto-detect SIMD width (AVX=4, SSE=2)
qv.normalize_inplace();              // SIMD under the hood
qv.rotate_vectors(vx, vy, vz);       // batch rotation

// 2. pack<quaternion> - low-level SIMD pack for quaternions
#include <optinum/simd/pack/quaternion.hpp>

pack<dp::mat::quaternion<double>, 4> qpack;  // 4 quaternions in AVX registers
qpack = qpack * other_qpack;                  // Hamilton product (SIMD)
auto logs = qpack.log();                      // Lie algebra (SIMD)

// 3. Owning container with transparent SIMD
#include <optinum/simd/quaternion.hpp>

simd::Quaternion<double, 8> rotations;
rotations.normalize_inplace();  // delegates to quaternion_view

// === NEW LIE GROUP BATCHED API ===

// SO3Batch uses quaternion_view internally
template <typename T, std::size_t N>
class SO3Batch {
    dp::mat::quaternion<T> quats_[N];  // Storage: array of quaternions
    
public:
    // All operations use quaternion_view for transparent SIMD
    auto as_view() { return simd::view(quats_); }
    
    static SO3Batch exp(const Matrix<T, 3, N>& omegas) {
        SO3Batch result;
        // Use pack<quaternion>::exp_pure for SIMD exp map
        // ...
        return result;
    }
    
    Matrix<T, 3, N> log() const {
        auto qv = simd::view(quats_);
        // Use pack.log() internally via view
        // ...
    }
    
    SO3Batch operator*(const SO3Batch& other) const {
        SO3Batch result;
        as_view().multiply_to(other.as_view(), result.quats_);
        return result;
    }
    
    void rotate(T* vx, T* vy, T* vz) const {
        as_view().rotate_vectors(vx, vy, vz);  // SIMD rotation
    }
    
    SO3Batch slerp(const SO3Batch& other, T t) const {
        SO3Batch result;
        as_view().slerp_to(other.as_view(), t, result.quats_);
        return result;
    }
};
```

**Key Point:** The `lie/batch/` module is a thin wrapper over `simd/` infrastructure:
- `simd::quaternion_view` handles SIMD dispatch automatically
- `simd::pack<quaternion>` provides low-level SIMD operations
- User works with `dp::mat::quaternion<T>` directly - no manual SIMD management

**Use cases:**
- Batch factor evaluation in SLAM
- Parallel ICP iterations
- Multi-sensor calibration

---

### Dependencies

**What We Have âœ…:**
- `simd::Matrix`, `simd::Vector` with SIMD âœ…
- `simd::pack<quaternion>` with full Lie ops âœ… (exp, log, slerp, Hamilton product)
- `simd::quaternion_view` - transparent SIMD over quaternion arrays âœ… **NEW**
- `simd::Quaternion<T,N>` - owning container with SIMD âœ… **SIMPLIFIED**
- `simd::view(quaternion_array)` - bridge for automatic SIMD dispatch âœ… **NEW**
- `dp::mat::quaternion<T>` â†” `dp::Quaternion` implicit conversion âœ…
- `lina::matmul`, `lina::transpose`, `lina::inverse` âœ…
- `simd::sin`, `simd::cos`, `simd::atan2`, `simd::sqrt` âœ…
- `lina::jacobian` for numerical derivatives âœ…
- `opti::GaussNewton`, `opti::LevenbergMarquardt` âœ…

**Quaternion SIMD Infrastructure (Ready for Lie Groups):**
```cpp
// User works with dp::mat::quaternion directly - SIMD is automatic
dp::mat::quaternion<double> quats[8];
auto qv = simd::view(quats);   // auto-detect width (AVX=4, SSE=2, scalar=1)

// All operations use SIMD internally:
qv.normalize_inplace();        // batch normalize
qv.conjugate_inplace();        // batch conjugate
qv.rotate_vectors(vx,vy,vz);   // batch rotation
qv.slerp_to(other, t, out);    // batch interpolation
qv.to_euler(r, p, y);          // batch conversion

// Spatial Quaternion also works (implicit conversion)
dp::Quaternion spatial_quats[8];
auto sv = simd::view(spatial_quats);  // same API
```

**What We Need to Add:**
- Core Lie group classes (SO2, SE2, SO3, SE3) - use `quaternion_view` for SO3
- Similarity groups (RxSO2, RxSO3, Sim2, Sim3)
- Algorithms (interpolate, average, spline)
- Batched versions - thin wrappers over `quaternion_view`

---

### SIMD Strategy

**Key Insight:** Use `dp::mat::quaternion<T>` as the storage type, then wrap with `simd::view()` for transparent SIMD acceleration. No need to think about SIMD width - it's auto-detected.

**Single Element:** `SO3<double>` uses scalar `dp::mat::quaternion<T>` operations

**Batched:** `SO3Batch<double, 8>` uses `simd::quaternion_view` for transparent SIMD

```cpp
// === Single rotation (scalar) ===
SO3<double> R = SO3<double>::exp(omega);
Vector3<double> p_rotated = R * p;

// === Batched rotations (transparent SIMD) ===
// Option 1: Use quaternion_view directly
dp::mat::quaternion<double> quats[8];
auto qv = simd::view(quats);  // auto-detect: AVX=4, SSE=2, scalar=1
qv.normalize_inplace();        // SIMD normalize
double vx[8], vy[8], vz[8];
qv.rotate_vectors(vx, vy, vz); // SIMD rotation

// Option 2: Use SO3Batch wrapper (delegates to quaternion_view)
SO3Batch<double, 8> Rs = SO3Batch<double, 8>::exp(omegas);
Rs.rotate(vx, vy, vz);  // internally uses quaternion_view

// === The SIMD happens automatically ===
// On AVX machine: processes 4 quaternions per SIMD op
// On SSE machine: processes 2 quaternions per SIMD op
// On scalar machine: falls back gracefully (W=1)
```

**SIMD Coverage:**
- Quaternion Hamilton product: 100% SIMD (via `pack<quaternion>`)
- SO3::exp (via quaternion): 95% SIMD (sin/cos are SIMD)
- SE3::exp: 90% SIMD (matmul, vector ops)
- Batched operations: 100% SIMD (via `quaternion_view`)
- Memory layout: AoS (user-friendly) â†” SoA (SIMD-friendly) conversion automatic

---

### API Design

```cpp
namespace optinum::lie {
    // Core groups
    template <typename T = double> class SO2;
    template <typename T = double> class SE2;
    template <typename T = double> class SO3;
    template <typename T = double> class SE3;
    
    // Similarity groups
    template <typename T = double> class RxSO2;
    template <typename T = double> class RxSO3;
    template <typename T = double> class Sim2;
    template <typename T = double> class Sim3;
    
    // Batched (SIMD)
    template <typename T, std::size_t N> class SO3Batch;
    template <typename T, std::size_t N> class SE3Batch;
    
    // Algorithms
    template <class G> G interpolate(const G& a, const G& b, typename G::Scalar t);
    template <class C> std::optional<typename C::value_type> average(const C& poses);
}

// Exposed in optinum:: namespace
namespace optinum {
    using lie::SO2;
    using lie::SE2;
    using lie::SO3;
    using lie::SE3;
    using lie::interpolate;
    using lie::average;
}
```

**Type aliases:**
```cpp
using SO2f = SO2<float>;
using SO2d = SO2<double>;
using SE3f = SE3<float>;
using SE3d = SE3<double>;
// etc.
```

---

### Example Usage

```cpp
#include <optinum/lie/lie.hpp>

using namespace optinum;

// === SO3 Examples ===

// Create rotation from axis-angle
Vector<double, 3> omega = {0.1, 0.2, 0.3};  // rotation vector
SO3<double> R = SO3<double>::exp(omega);

// Rotate a point
Vector<double, 3> p = {1, 0, 0};
Vector<double, 3> p_rotated = R * p;

// Compose rotations
SO3<double> R2 = SO3<double>::rotZ(M_PI / 4);
SO3<double> R_composed = R * R2;

// Get rotation matrix
Matrix<double, 3, 3> mat = R.matrix();

// Interpolation (slerp on manifold)
SO3<double> R_mid = interpolate(R, R2, 0.5);

// === SE3 Examples ===

// Create pose
SE3<double> T = SE3<double>(R, Vector<double, 3>{1, 2, 3});

// Transform point
Vector<double, 3> p_world = T * p;

// Inverse
SE3<double> T_inv = T.inverse();

// Log map (for optimization)
Vector<double, 6> twist = T.log();

// === Optimization Example ===

// Camera pose optimization
auto residuals = [&](const Vector<double, 6>& xi) {
    SE3<double> pose = SE3<double>::exp(xi);
    // Compute reprojection errors...
    return errors;
};

LevenbergMarquardt<double> optimizer;
auto result = optimizer.optimize(residuals, Vector<double, 6>::Zero());
SE3<double> optimal_pose = SE3<double>::exp(result.x);

// === Batched Operations ===

// Process 8 rotations at once (AVX)
Matrix<double, 3, 8> omegas;  // 8 rotation vectors
SO3Batch<double, 8> Rs = SO3Batch<double, 8>::exp(omegas);

Matrix<double, 3, 8> points;  // 8 points to rotate
Matrix<double, 3, 8> rotated = Rs.rotate(points);  // All 8 rotated in parallel
```

---

### Implementation Priority

| Phase | Component | Effort | Priority | Status |
|-------|-----------|--------|----------|--------|
| 0.7-pre | **Quaternion SIMD Infrastructure** | - | â­â­â­â­â­ | âœ… DONE |
| | `pack<quaternion>` | - | - | âœ… 636 lines, 20 tests |
| | `quaternion_view` | - | - | âœ… 380 lines, 19 tests |
| | `bridge.hpp` quaternion support | - | - | âœ… 110 lines added |
| 0.7a | **SO2** | 1 day | â­â­â­â­â­ | âœ… DONE - 320 lines, 21 tests, 687 assertions |
| 0.7b | **SE2** | 1-2 days | â­â­â­â­ | âœ… DONE - 450 lines, 22 tests, 724 assertions |
| 0.7c | **SO3** | 2-3 days | â­â­â­â­â­ | ğŸ”² TODO |
| 0.7d | **SE3** | 2-3 days | â­â­â­â­â­ | ğŸ”² TODO |
| 0.7e | **RxSO2/3, Sim2/3** | 3-4 days | â­â­â­ | ğŸ”² TODO |
| 0.7f | **Algorithms** | 2 days | â­â­â­â­ | ğŸ”² TODO |
| 0.7g | **Batched SIMD** | 1-2 days | â­â­â­â­â­ | ğŸ”² TODO (foundation ready) |

**Total Estimate:** 2-3 weeks (reduced from original due to SIMD foundation being ready)

---

### Testing Strategy

**Unit Tests:**
- Group axioms: identity, inverse, closure, associativity
- exp/log round-trip consistency
- hat/vee inverse relationship
- Jacobian correctness vs finite differences

**Numerical Tests:**
- Stability near singularities (small angles)
- Quaternion normalization preservation
- Rotation matrix orthogonality

**Integration Tests:**
- Bundle adjustment with SE3
- Rotation averaging with SO3
- Pose graph optimization

---

### Sophus Function Coverage

All major Sophus functions will be implemented:

| Sophus File | Our File | Functions |
|-------------|----------|-----------|
| `so2.hpp` | `groups/so2.hpp` | exp, log, hat, vee, Adj, generators, derivatives |
| `se2.hpp` | `groups/se2.hpp` | exp, log, hat, vee, Adj, Jacobians, derivatives |
| `so3.hpp` | `groups/so3.hpp` | exp, log, hat, vee, Adj, leftJacobian, derivatives |
| `se3.hpp` | `groups/se3.hpp` | exp, log, hat, vee, Adj, Jacobians, derivatives |
| `rxso2.hpp` | `groups/rxso2.hpp` | scale, rotation extraction |
| `rxso3.hpp` | `groups/rxso3.hpp` | scale, rotation extraction |
| `sim2.hpp` | `groups/sim2.hpp` | similarity transforms 2D |
| `sim3.hpp` | `groups/sim3.hpp` | similarity transforms 3D |
| `interpolate.hpp` | `algorithms/interpolate.hpp` | geodesic interpolation |
| `average.hpp` | `algorithms/average.hpp` | biinvariant mean |
| `spline.hpp` | `algorithms/spline.hpp` | Lie group splines |
| `geometry.hpp` | `algorithms/geometry.hpp` | pose/plane/line utilities |
| `rotation_matrix.hpp` | `core/rotation_matrix.hpp` | isOrthogonal, makeRotationMatrix |
| `common.hpp` | `core/constants.hpp` | epsilon, pi |

**Not porting:**
- `ceres_manifold.hpp` - Ceres-specific
- `ceres_typetraits.hpp` - Ceres-specific
- Eigen::Map specializations - we use datapod, not Eigen

---

### Next Steps

1. âœ… Finalize plan (this document)
2. âœ… Quaternion SIMD infrastructure (`quaternion_view`, `pack<quaternion>`, bridge)
3. âœ… Implement SO2 + tests - **DONE Dec 28, 2025**
4. âœ… Implement SE2 + tests - **DONE Dec 28, 2025**
5. ğŸ”² Implement SO3 + tests (use `dp::mat::quaternion<T>` + `quaternion_view` for batched)
6. ğŸ”² Implement SE3 + tests
7. ğŸ”² Implement algorithms (interpolate, average)
8. ğŸ”² Implement SO3Batch/SE3Batch (thin wrappers over `quaternion_view`)
9. ğŸ”² Add similarity groups if needed
10. ğŸ”² Create examples and documentation

