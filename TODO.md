# OPTINUM - Optimization & Numerics Library

> **HEADER-ONLY C++20 LIBRARY** - No compilation required, just `#include <optinum/optinum.hpp>`

---

## Module Status

| Module | Status | Description |
|--------|--------|-------------|
| `simd/` | **âœ… COMPLETE** | SIMD operations, views, pack<T,W>, math functions (40+) |
| `lina/` | **âœ… COMPLETE** | Linear algebra (110 functions, all major decompositions + Jacobian) |
| `opti/` | **âœ… PHASE 0 DONE** | 6 optimizers complete (GD, Momentum, RMSprop, Adam, GN, LM) |
| **API** | **âœ… COMPLETE** | Unified optinum:: namespace (85+ functions) |

**Test Status:** 71/71 tests passing âœ… (63 base + 8 quasi-Newton)

---

## ğŸ¯ Current Implementation Status

### âœ… COMPLETE - SIMD Module (simd/)

**Core Infrastructure:**
- âœ… `pack<T,W>` with SSE/AVX/AVX-512/NEON support
- âœ… `mask<T,W>` for conditional operations
- âœ… Views: Vector, Matrix, Tensor (non-owning, zero-copy)
- âœ… Slicing: diagonal, filter, random_access
- âœ… **Dynamic size support** - Runtime-sized vectors/matrices

**40+ SIMD Math Functions:**
- âœ… Exponential/Log: exp, log, sqrt, pow, exp2, log2, log10, cbrt, etc.
- âœ… Trigonometric: sin, cos, tan, asin, acos, atan, atan2
- âœ… Hyperbolic: sinh, cosh, tanh, asinh, acosh, atanh
- âœ… Rounding: ceil, floor, round, trunc
- âœ… Utility: abs, clamp, hypot, isnan, isinf
- âœ… Special: erf, tgamma, lgamma

**Algorithms:**
- âœ… Elementwise: add, sub, mul, div, fill, copy, axpy, scale
- âœ… Reductions: sum, min, max, dot, norm
- âœ… Backend: Specialized 2x2/3x3/4x4 kernels (32-243x speedup)

---

### âœ… COMPLETE - Linear Algebra Module (lina/)

**107 Functions Implemented:**

**Basic Operations (20):**
- âœ… matmul, transpose, inverse, determinant, norm, trace
- âœ… adjoint, cofactor, cond, rcond, rank
- âœ… is_symmetric, is_hermitian, is_positive_definite

**Decompositions (5):**
- âœ… LU (with partial pivoting)
- âœ… QR (Householder reflections)
- âœ… SVD (one-sided Jacobi)
- âœ… Cholesky (SPD matrices)
- âœ… Eigendecomposition (symmetric)

**Solvers (2):**
- âœ… solve (Ax = b via LU)
- âœ… lstsq (least squares via QR)

**Advanced (6):**
- âœ… pinv (pseudo-inverse via SVD)
- âœ… null (null space via SVD)
- âœ… orth (orthonormal basis via QR)
- âœ… kron (Kronecker product)
- âœ… permute (tensor permutations)
- âœ… einsum (Einstein summation)

**Calculus/Differentiation:**
- âœ… **jacobian** - Finite-difference Jacobian matrix computation (forward/central)
- âœ… **gradient** - Finite-difference gradient (optimized for scalar functions)
- âœ… **jacobian_error** - Helper for comparing numerical vs analytical Jacobians
- ğŸ”² hessian - Finite-difference Hessian (future)

**All with SIMD acceleration (60-95% SIMD coverage)**

---

### ğŸš§ IN PROGRESS - Optimization Module (opti/)

**âœ… IMPLEMENTED (4 optimizers):**

1. **Vanilla Gradient Descent** (`vanilla_update.hpp`) âœ…
   - Basic gradient descent: `x -= Î± * âˆ‡f(x)`
   - Stateless, simple, benchmark baseline

2. **Momentum** (`momentum_update.hpp`) âœ…
   - Classical momentum (Rumelhart 1986)
   - SIMD-optimized: 2.1x speedup over scalar
   - Supports both fixed and Dynamic sizes

3. **RMSprop** (`rmsprop_update.hpp`) âœ…
   - Adaptive learning rates (Hinton 2012)
   - SIMD-optimized: 5.8x speedup over scalar
   - Supports both fixed and Dynamic sizes

4. **Adam** (`adam_update.hpp`) âœ…
   - Adaptive moment estimation (Kingma & Ba 2014)
   - SIMD-optimized: 3.6x speedup over scalar
   - Supports both fixed and Dynamic sizes
   - Bias correction for moments

**Infrastructure:**
- âœ… `GradientDescent` optimizer template
- âœ… Callback system (`NoCallback`, custom callbacks)
- âœ… Decay policies (`NoDecay`)
- âœ… Function traits and type system
- âœ… Test problems (Sphere function)
- âœ… **Dynamic size support** - All optimizers work with runtime-sized problems

**Performance:**
- âœ… SIMD-accelerated updates (2-6x faster than scalar)
- âœ… Zero-copy views over datapod types
- âœ… Fixed-size: 100% performance (compile-time SIMD)
- âœ… Dynamic-size: ~90% performance (runtime SIMD dispatch)

---

## ğŸ“‹ TODO - Optimization Components to Implement

### **âœ… Phase 0: COMPLETE - Core Infrastructure from graphix**

**Status:** ALL 3 COMPONENTS IMPLEMENTED AND TESTED âœ…

#### âœ… 0a. **Finite-Difference Jacobian** - DONE
- **File:** `include/optinum/lina/basic/jacobian.hpp`
- **Complexity:** â­â­ Medium (~150 lines)
- **Impact:** Core infrastructure for nonlinear least squares
- **Module:** Linear Algebra (calculus operations)
- **Algorithm:** 
  - Forward difference: `J[i,j] = (f_i(x + hÂ·e_j) - f_i(x)) / h`
  - Central difference: `J[i,j] = (f_i(x + hÂ·e_j) - f_i(x - hÂ·e_j)) / (2h)` (more accurate)
- **Functions:**
  ```cpp
  // Compute Jacobian matrix for f: R^n -> R^m
  lina::jacobian(f, x, h=1e-8, central=true) -> Matrix<T, Dynamic, N>
  
  // Optimized gradient for scalar f: R^n -> R
  lina::gradient(f, x, h=1e-8, central=true) -> Vector<T, N>
  ```
- **Source:** Ported from `graphix/src/graphix/factor/nonlinear/nonlinear_factor.cpp::linearize()`
- **âœ… Implemented:** `include/optinum/lina/basic/jacobian.hpp` (210 lines)
- **âœ… Tests:** 15/15 passing - `test/lina/basic/jacobian_test.cpp`
- **âœ… Features:** Forward/central differences, gradient specialization, error checking

#### âœ… 0b. **Gauss-Newton Optimizer** - DONE
- **File:** `include/optinum/opti/quasi_newton/gauss_newton.hpp`
- **Complexity:** â­â­ Medium (~200 lines)
- **Impact:** Fast solver for nonlinear least squares (robotics, vision, SLAM)
- **Module:** Optimization (second-order methods)
- **Algorithm:** 
  ```
  For each iteration:
    1. Compute Jacobian J and residual b = f(x)
    2. Solve: (J^T * J) * delta = -J^T * b  (normal equations)
    3. Update: x += delta
    4. Check convergence
  ```
- **Dependencies:** 
  - Needs `lina::jacobian()` to compute J
  - Needs `lina::matmul()` for J^T * J and J^T * b
  - Needs `lina::solve()` or Cholesky for symmetric system
- **Source:** Ported from `graphix/include/graphix/factor/nonlinear/gauss_newton.hpp`
- **âœ… Implemented:** `include/optinum/opti/quasi_newton/gauss_newton.hpp` (650+ lines)
- **âœ… Tests:** 9/9 passing - `test/opti/quasi_newton/gauss_newton_test.cpp`
- **âœ… Features:** Multiple solvers, line search, convergence criteria, verbose mode
- **âœ… Example:** `examples/gauss_newton_demo.cpp` (curve fitting, circle fitting, Rosenbrock)

#### âœ… 0c. **Levenberg-Marquardt Optimizer** - DONE
- **File:** `include/optinum/opti/quasi_newton/levenberg_marquardt.hpp`
- **Complexity:** â­â­â­ Medium-Hard (~250 lines)
- **Impact:** More robust than Gauss-Newton, industry standard (scipy, ceres)
- **Module:** Optimization (second-order methods)
- **Algorithm:**
  ```
  For each iteration:
    1. Compute J and b = f(x)
    2. Solve: (J^T * J + Î»*I) * delta = -J^T * b  (damped normal equations)
    3. Try step: x_new = x + delta
    4. If error decreased: accept, Î» /= 10 (approach Gauss-Newton)
       Else: reject, Î» *= 10 (approach gradient descent)
    5. Check convergence
  ```
- **Parameters:**
  - `lambda_init = 1e-3` - Initial damping
  - `lambda_factor = 10.0` - Adjustment factor
  - `min_lambda = 1e-7, max_lambda = 1e7` - Bounds
- **Dependencies:** Same as Gauss-Newton + diagonal addition for damping
- **Source:** Ported from `graphix/include/graphix/factor/nonlinear/levenberg_marquardt.hpp`
- **âœ… Implemented:** `include/optinum/opti/quasi_newton/levenberg_marquardt.hpp` (545 lines)
- **âœ… Tests:** 8/8 passing - `test/opti/quasi_newton/levenberg_marquardt_test.cpp`  
- **âœ… Features:** Adaptive damping, robust to poor initialization, handles ill-conditioned problems
- **âœ… Example:** `examples/levenberg_marquardt_demo.cpp` (robustness demo, bundle adjustment)

**âœ… Phase 0 Complete!**
- âœ… All 3 components implemented (Jacobian, Gauss-Newton, Levenberg-Marquardt)
- âœ… 32/32 new tests passing (15 Jacobian + 9 GN + 8 LM)
- âœ… Production-ready, ported from graphix
- âœ… API exposed in `optinum::` namespace
- âœ… Examples and demos created

**Impact:**
- Optinum now has **second-order methods** (much faster than gradient descent)
- Convergence: 5-10 iterations (vs 100+ for gradient descent)
- Ready for robotics, computer vision, SLAM, bundle adjustment
- Industry-standard algorithms (used in Ceres, g2o, GTSAM)

---

### **Tier 1: Essential (Must Have) - 5 optimizers**

#### 1. **Nesterov Momentum (NAG)** - HIGHEST PRIORITY â­â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/nesterov_momentum_update.hpp`
- **Complexity:** â­ Easy (~60 lines)
- **Impact:** O(1/kÂ²) convergence, used in 40% of momentum papers
- **Algorithm:** Lookahead gradient: `v = Î¼v - Î±âˆ‡f(x + Î¼v); x = x + v`
- **Reference:** Nesterov (1983) "A Method Of Solving A Convex Programming Problem"
- **SIMD:** Same pattern as Momentum (already implemented)

#### 2. **AdaGrad** â­â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/adagrad_update.hpp`
- **Complexity:** â­ Easy (~80 lines)
- **Impact:** Foundation for all adaptive methods (6000+ citations)
- **Algorithm:** `G += gÂ²; x -= Î± * g / (âˆšG + Îµ)` (accumulate squared gradients)
- **Reference:** Duchi et al. (2011) "Adaptive Subgradient Methods"
- **SIMD:** Element-wise ops (like RMSprop)

#### 3. **AdaDelta** â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/adadelta_update.hpp`
- **Complexity:** â­ Easy (~100 lines)
- **Impact:** Fixes AdaGrad's monotonic decay, popular in NLP/RNNs
- **Algorithm:** No manual learning rate! Uses RMS of gradients
- **Reference:** Zeiler (2012) "ADADELTA: An Adaptive Learning Rate Method"
- **SIMD:** Same as RMSprop + one extra EMA

#### 4. **AMSGrad** â­â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/amsgrad_update.hpp`
- **Complexity:** â­ Trivial (~20 lines - just modify Adam!)
- **Impact:** Fixes Adam's convergence issues, proven to converge
- **Algorithm:** `v_hat = max(v_hat, v)` (non-increasing second moment)
- **Reference:** Reddi et al. (2018) "On the Convergence of Adam and Beyond"
- **SIMD:** Change 1 line in Adam implementation!

#### 5. **L-BFGS** â­â­â­â­â­
- **File:** `include/optinum/opti/quasi_newton/lbfgs.hpp`
- **Complexity:** â­â­â­ Hard (~400 lines)
- **Impact:** THE quasi-Newton method, industry standard
- **Algorithm:** Limited-memory BFGS with line search
- **Reference:** Liu & Nocedal (1989) "On the Limited Memory BFGS Method"
- **SIMD:** Vector updates, dot products (60% SIMD coverage)
- **Note:** Requires line search implementation

---

### **Tier 2: Very Important (Should Have) - 3 optimizers**

#### 6. **Lookahead** â­â­â­â­
- **File:** `include/optinum/opti/meta/lookahead.hpp`
- **Complexity:** â­ Easy (~80 lines)
- **Impact:** Meta-optimizer wrapper, improves ANY base optimizer
- **Algorithm:** Slow weights = slow + Î±(fast - slow) every k steps
- **Reference:** Zhang et al. (2019) "Lookahead Optimizer"
- **SIMD:** Trivial (just weight averaging)
- **Usage:** `Lookahead<Adam>`, `Lookahead<SGD>`, etc.

#### 7. **AdaBound / AMSBound** â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/adabound_update.hpp`
- **Complexity:** â­ Easy (~100 lines)
- **Impact:** Transitions from Adam â†’ SGD during training
- **Algorithm:** Adam with bounded learning rates [0.1-Î±, Î±]
- **Reference:** Luo et al. (2019) "Adaptive Gradient Methods with Dynamic Bound"
- **SIMD:** Same as Adam + clipping

#### 8. **Yogi** â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/yogi_update.hpp`
- **Complexity:** â­ Easy (~90 lines)
- **Impact:** Google's Adam improvement, used in production
- **Algorithm:** `v = v - (1-Î²â‚‚) * sign(v - gÂ²) * gÂ²` (gentler decay)
- **Reference:** Zaheer et al. (2018) "Adaptive Methods for Nonconvex Optimization"
- **SIMD:** Same as Adam, just change v update

---

### **Tier 3: Nice to Have (Specialized) - 2 optimizers**

#### 9. **NAdam** â­â­â­
- **File:** `include/optinum/opti/gradient/update_policies/nadam_update.hpp`
- **Complexity:** â­ Easy (~90 lines)
- **Impact:** Nesterov + Adam, popular in Keras
- **Algorithm:** Adam with Nesterov momentum
- **Reference:** Dozat (2016) "Incorporating Nesterov Momentum into Adam"
- **SIMD:** Combine Adam + Nesterov patterns

#### 10. **SWATS** â­â­
- **File:** `include/optinum/opti/meta/swats.hpp`
- **Complexity:** â­â­ Medium (~150 lines)
- **Impact:** Auto-switches Adam â†’ SGD
- **Algorithm:** Start with Adam, switch to SGD when stable
- **Reference:** Keskar & Socher (2017) "Improving Generalization Performance"
- **SIMD:** Same as base optimizers + switching logic

---

## ğŸ“Š Implementation Priority Summary

| Rank | Component | Type | Difficulty | Lines | Impact | Priority |
|------|-----------|------|-----------|-------|---------|----------|
| **âœ… Phase 0: COMPLETE (from graphix)** |
| âœ… 0a | **Jacobian** | Lina | â­â­ Medium | 210 | â­â­â­â­â­ | **DONE** |
| âœ… 0b | **Gauss-Newton** | Opti | â­â­ Medium | 650+ | â­â­â­â­â­ | **DONE** |
| âœ… 0c | **Levenberg-Marquardt** | Opti | â­â­â­ Hard | 545 | â­â­â­â­â­ | **DONE** |
| **Tier 1: Essential First-Order** |
| 1 | **Nesterov** | Opti | â­ Easy | ~60 | â­â­â­â­â­ | **MUST** |
| 2 | **AdaGrad** | Opti | â­ Easy | ~80 | â­â­â­â­â­ | **MUST** |
| 3 | **AdaDelta** | Opti | â­ Easy | ~100 | â­â­â­â­ | **MUST** |
| 4 | **AMSGrad** | Opti | â­ Trivial | ~20 | â­â­â­â­ | **MUST** |
| 5 | **L-BFGS** | Opti | â­â­â­ Hard | ~400 | â­â­â­â­â­ | **MUST** |
| **Tier 2: Very Important** |
| 6 | **Lookahead** | Opti | â­ Easy | ~80 | â­â­â­â­ | High |
| 7 | **AdaBound** | Opti | â­ Easy | ~100 | â­â­â­ | High |
| 8 | **Yogi** | Opti | â­ Easy | ~90 | â­â­â­ | Medium |
| **Tier 3: Nice to Have** |
| 9 | **NAdam** | Opti | â­ Easy | ~90 | â­â­â­ | Medium |
| 10 | **SWATS** | Opti | â­â­ Medium | ~150 | â­â­ | Low |

**Total estimated effort:** 
- **âœ… Phase 0:** COMPLETE - Jacobian, Gauss-Newton, Levenberg-Marquardt
- **Tiers 1-3:** 6-8 days (10 optimizers remaining)
- **Remaining:** 6-8 days for Tiers 1-3

---

## ğŸ¯ Recommended Implementation Order

### âœ… Phase 0: Core Infrastructure from Graphix - **COMPLETE!** âœ…
1. âœ… **Jacobian computation** (210 lines, 15 tests passing)
   - Created `lina/basic/jacobian.hpp`
   - Implemented `jacobian()` and `gradient()` with forward/central differences
   - Added `jacobian_error()` helper for validation
2. âœ… **Gauss-Newton optimizer** (650+ lines, 9 tests passing)
   - Created `opti/quasi_newton/gauss_newton.hpp`
   - Ported algorithm from graphix, adapted to SIMD types
   - Example demo: curve fitting, circle fitting, Rosenbrock
3. âœ… **Levenberg-Marquardt optimizer** (545 lines, 8 tests passing)
   - Created `opti/quasi_newton/levenberg_marquardt.hpp`
   - Implemented damped GN with adaptive Î»
   - Example demo: robustness comparison, bundle adjustment

**Phase 0 Success:**
- All 32 tests passing (15 Jacobian + 9 GN + 8 LM)
- Production-ready, ported from proven graphix code
- API fully exposed in `optinum::` namespace
- Comprehensive examples and demos

---

### Phase 1: Quick Wins (1-2 days)
4. **AMSGrad** - 20 lines, modify Adam
5. **Nesterov** - 60 lines, huge impact
6. **AdaGrad** - 80 lines, foundation

### Phase 2: Core Adaptive Methods (2-3 days)
7. **AdaDelta** - 100 lines, popular
8. **NAdam** - 90 lines, Nesterov + Adam
9. **Yogi** - 90 lines, Google variant

### Phase 3: Meta-Optimizers (1-2 days)
10. **Lookahead** - 80 lines, wrapper
11. **AdaBound** - 100 lines, hybrid

### Phase 4: Advanced (3-5 days)
12. **L-BFGS** - 400 lines, quasi-Newton
13. **SWATS** - 150 lines, auto-switching

---

## ğŸ—ï¸ Architecture & Design

### Module Structure

```
include/optinum/lina/
â”œâ”€â”€ lina.hpp                          # Main lina header
â”œâ”€â”€ basic/
â”‚   â”œâ”€â”€ ...                           # âœ… Existing (matmul, det, etc.)
â”‚   â””â”€â”€ jacobian.hpp                  # âœ… DONE: Jacobian & gradient (Phase 0)
â””â”€â”€ ...

include/optinum/opti/
â”œâ”€â”€ opti.hpp                          # Main header (expose all to optinum::)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ types.hpp                     # âœ… Result types, OptimizationResult
â”‚   â”œâ”€â”€ function.hpp                  # âœ… Function traits, concepts
â”‚   â””â”€â”€ callbacks.hpp                 # âœ… Callback system
â”œâ”€â”€ decay/
â”‚   â””â”€â”€ no_decay.hpp                  # âœ… Learning rate decay (none for now)
â”œâ”€â”€ gradient/
â”‚   â”œâ”€â”€ gradient_descent.hpp          # âœ… Main GD optimizer template
â”‚   â””â”€â”€ update_policies/
â”‚       â”œâ”€â”€ vanilla_update.hpp        # âœ… Basic SGD
â”‚       â”œâ”€â”€ momentum_update.hpp       # âœ… Classical momentum
â”‚       â”œâ”€â”€ nesterov_momentum_update.hpp  # ğŸ”² TODO: Nesterov (Tier 1)
â”‚       â”œâ”€â”€ rmsprop_update.hpp        # âœ… RMSprop
â”‚       â”œâ”€â”€ adam_update.hpp           # âœ… Adam
â”‚       â”œâ”€â”€ amsgrad_update.hpp        # ğŸ”² TODO: AMSGrad (Tier 1)
â”‚       â”œâ”€â”€ adagrad_update.hpp        # ğŸ”² TODO: AdaGrad (Tier 1)
â”‚       â”œâ”€â”€ adadelta_update.hpp       # ğŸ”² TODO: AdaDelta (Tier 1)
â”‚       â”œâ”€â”€ nadam_update.hpp          # ğŸ”² TODO: NAdam (Tier 3)
â”‚       â”œâ”€â”€ yogi_update.hpp           # ğŸ”² TODO: Yogi (Tier 2)
â”‚       â””â”€â”€ adabound_update.hpp       # ğŸ”² TODO: AdaBound (Tier 2)
â”œâ”€â”€ meta/
â”‚   â”œâ”€â”€ lookahead.hpp                 # ğŸ”² TODO: Lookahead wrapper (Tier 2)
â”‚   â””â”€â”€ swats.hpp                     # ğŸ”² TODO: SWATS (Tier 3)
â”œâ”€â”€ quasi_newton/                     # âœ… Directory for second-order methods
â”‚   â”œâ”€â”€ gauss_newton.hpp              # âœ… DONE: Phase 0 (from graphix) - 650+ lines
â”‚   â”œâ”€â”€ levenberg_marquardt.hpp       # âœ… DONE: Phase 0 (from graphix) - 545 lines
â”‚   â””â”€â”€ lbfgs.hpp                     # ğŸ”² TODO: L-BFGS (Tier 1)
â””â”€â”€ problem/
    â”œâ”€â”€ sphere.hpp                    # âœ… Test function
    â”œâ”€â”€ rosenbrock.hpp                # ğŸ”² TODO: Classic test
    â”œâ”€â”€ rastrigin.hpp                 # ğŸ”² TODO: Multimodal test
    â””â”€â”€ ackley.hpp                    # ğŸ”² TODO: Multimodal test
```

---

## ğŸ”§ Implementation Guidelines

### For All New Optimizers:

1. **File Location:**
   - Update policies: `opti/gradient/update_policies/`
   - Meta-optimizers: `opti/meta/`
   - Quasi-Newton: `opti/quasi_newton/`

2. **Required Interface (for update policies):**
   ```cpp
   struct MyOptimizer {
       // State variables
       std::vector<double> state;
       
       // Parameters
       double param1 = default_value;
       
       // Constructor
       explicit MyOptimizer(double p1 = default) : param1(p1) {}
       
       // Update function (MUST support both fixed and Dynamic sizes)
       template <typename T, std::size_t N>
       void update(simd::Vector<T, N> &x, T step_size, 
                   const simd::Vector<T, N> &gradient) noexcept {
           const std::size_t n = x.size(); // Get runtime size
           
           // Initialize state on first call
           if (state.size() != n) {
               state.resize(n, T{0});
           }
           
           // SIMD dual path pattern
           if constexpr (N == simd::Dynamic) {
               // Runtime SIMD path
               const std::size_t W = simd::backend::preferred_simd_lanes_runtime<T>();
               constexpr std::size_t pack_width = std::is_same_v<T, double> ? 4 : 8;
               using pack_t = simd::pack<T, pack_width>;
               // ... SIMD loops with runtime bounds ...
           } else {
               // Compile-time SIMD path
               constexpr std::size_t W = simd::backend::preferred_simd_lanes<T, N>();
               using pack_t = simd::pack<T, W>;
               // ... SIMD loops with compile-time bounds ...
           }
       }
       
       // Reset state
       void reset() noexcept { state.clear(); }
       
       // Initialize (called by GradientDescent)
       template <typename T, std::size_t N>
       void initialize(std::size_t n) noexcept { state.clear(); }
   };
   ```

3. **SIMD Requirements:**
   - ALL updates must use SIMD (2-6x speedup expected)
   - Support both fixed-size (compile-time) and Dynamic (runtime) vectors
   - Use `if constexpr (N == Dynamic)` to dispatch between paths
   - Never use `N` directly when it might be Dynamic - use `x.size()`

4. **Testing:**
   - Add test in `test/opti/optimizer_comparison_test.cpp`
   - Test both fixed-size and Dynamic-size vectors
   - Verify convergence on Sphere function
   - Compare results between fixed and Dynamic

5. **API Exposure:**
   - Add to `include/optinum/opti/opti.hpp`
   - Expose in `optinum::` namespace via `include/optinum/optinum.hpp`
   - Example: `using Nesterov = opti::NesterovUpdate;`

---

## ğŸ“ Reference Implementations

**Check ensmallen for algorithms:**
- Location: `./xtra/ensmallen/include/ensmallen_bits/`
- Use for understanding math, NOT copying code (different license)
- Our implementation: SIMD-accelerated, dual compile/runtime paths

**Key Differences from ensmallen:**
- âœ… We use SIMD (2-6x faster)
- âœ… We support both fixed and Dynamic sizes
- âœ… We're header-only (easier to integrate)
- âœ… We use datapod types (cleaner ownership)

---

## ğŸ“ˆ Success Metrics

**When all 10 optimizers are done:**

âœ… **Feature Parity:**
- Same core optimizers as PyTorch/TensorFlow
- All major adaptive methods covered
- Both first-order and quasi-Newton available

âœ… **Performance:**
- 2-6x faster than scalar (via SIMD)
- ~90% performance for Dynamic vs fixed sizes
- Zero-copy views over datapod

âœ… **Flexibility:**
- Works with compile-time AND runtime-sized problems
- Meta-optimizers (Lookahead, SWATS) wrap any base optimizer
- Easy to add custom optimizers

âœ… **Quality:**
- All tests passing (60+ tests)
- Proven convergence on test problems
- Production-ready code

---

## ğŸš€ After Optimization Module Complete

**Immediate Value (Phase 0 + Tier 1):**
After implementing Phase 0 and Tier 1, we'll have:
- âœ… 4 first-order optimizers (Vanilla, Momentum, RMSprop, Adam)
- âœ… 3 second-order optimizers (Gauss-Newton, LM, L-BFGS)
- âœ… 4 more first-order variants (Nesterov, AdaGrad, AdaDelta, AMSGrad)
- âœ… Jacobian/gradient computation infrastructure

**This is production-ready for 90% of use cases!**

---

**Future Expansion (Optional):**
- [ ] More test problems (Rosenbrock, Rastrigin, Ackley)
- [ ] Learning rate schedulers (cosine annealing, step decay)
- [ ] Gradient clipping callbacks
- [ ] Line search algorithms (Armijo, Wolfe) - needed for L-BFGS
- [ ] Stochastic methods (SVRG, SARAH)
- [ ] Evolutionary algorithms (CMA-ES, DE, PSO)
- [ ] Constrained optimization (Augmented Lagrangian)

**Not Priority:**
- These are nice-to-have but not needed for MVP
- Focus on Phase 0 + Tier 1 first (most impact)
- Can add later based on user demand

---

## ğŸ“š Dependencies

- **C++20** or later
- **datapod** v0.0.10 (fetched automatically)
- **doctest** (for tests, fetched automatically)
- **Optional:** AVX2/AVX-512 for maximum SIMD performance

---

## ğŸ”— References

**Optimization:**
- ensmallen: https://github.com/mlpack/ensmallen
- PyTorch optimizers: https://pytorch.org/docs/stable/optim.html
- Adam paper: Kingma & Ba (2014) https://arxiv.org/abs/1412.6980
- L-BFGS: Liu & Nocedal (1989)

**SIMD:**
- Intel Intrinsics Guide: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/
- SLEEF: https://sleef.org/

**Testing:**
- Test Problems: https://www.sfu.ca/~ssurjano/optimization.html

---

## ğŸ“ Notes

- All modules follow the same dual-path SIMD pattern (compile-time + runtime)
- This TODO tracks ONLY what's left to do (not what's done)
- See git history for full development timeline
- Current focus: Implementing the 10 priority optimizers

---

## ğŸ“¦ Summary: Graphix Integration (Phase 0)

**What we're porting from `../graphix`:**

1. **Jacobian computation** â†’ `lina/basic/jacobian.hpp`
   - Finite-difference Jacobian for vector functions
   - Optimized gradient for scalar functions
   - Both forward and central differences

2. **Gauss-Newton optimizer** â†’ `opti/quasi_newton/gauss_newton.hpp`
   - Nonlinear least squares solver
   - Fast convergence (5-10 iterations typical)
   - Production-ready (used in graphix SLAM)

3. **Levenberg-Marquardt** â†’ `opti/quasi_newton/levenberg_marquardt.hpp`
   - Damped Gauss-Newton (more robust)
   - Adaptive trust region (adjusts Î»)
   - Industry standard (scipy, ceres, g2o)

**Why these 3?**
- Already proven in production (graphix)
- Fill critical gap (second-order methods)
- Needed for robotics, vision, SLAM applications
- Complement our first-order optimizers

**Total effort:** ~1.5-2 days (12-16 hours)

**After Phase 0:** Optinum will have both first-order (GD, Adam) AND second-order (GN, LM) optimizers!

---

---

## ğŸ‰ PHASE 0 MILESTONE ACHIEVED - December 27, 2025

**Major Achievement:** Second-order optimization methods now available!

**What's New:**
- âœ… **3 new components** - Jacobian, Gauss-Newton, Levenberg-Marquardt
- âœ… **32 new tests** - All passing (15 + 9 + 8)
- âœ… **1,405 lines** of production code
- âœ… **2 example demos** - gauss_newton_demo.cpp, levenberg_marquardt_demo.cpp
- âœ… **Full API exposure** - Available via `optinum::jacobian`, `optinum::GaussNewton<>`, `optinum::LevenbergMarquardt<>`

**Performance:**
- Gauss-Newton: 5-10 iterations typical (vs 100+ for gradient descent)
- Levenberg-Marquardt: Robust to poor initialization
- SIMD-accelerated Jacobian computation
- Production-ready for robotics, vision, SLAM

**Test Status:** 71/71 tests passing âœ… (100% pass rate)

---

**Last Updated:** December 27, 2025 - Phase 0 Complete!
